{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python39\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from facenet_pytorch import MTCNN#https://github.com/timesler/facenet-pytorch\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import math\n",
    "import glob\n",
    "import torch \n",
    "from torchvision import transforms\n",
    "from facenet_pytorch import InceptionResnetV1, fixed_image_standardization\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "device =  torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(thresholds= [0.7, 0.7, 0.8],min_face_size=80,keep_all=True, device = device)#Thresholds is for 3 nets P, R and O. Defaut value are [0.6, 0.7, 0.7] \n",
    "#Keep_all to let model know if we want to detect and return all the face in the capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH = 640#size of capture\n",
    "HEIGHT = 480\n",
    "prev_frame_time = 0\n",
    "new_frame_time = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[652.8506469726562 233.7052001953125 856.620849609375 491.2525634765625]\n",
      "[223.8394012451172 236.81600952148438 405.4974365234375 457.4383239746094]\n",
      "[1008.12109375 103.744140625 1119.513427734375 233.1702423095703]\n",
      "[653.5598754882812 234.71237182617188 856.4798583984375 490.90655517578125]\n",
      "[225.12283325195312 235.51577758789062 407.8699951171875\n",
      " 458.13665771484375]\n",
      "[1007.7916259765625 103.9172592163086 1119.42626953125 232.6063995361328]\n",
      "[658.7639770507812 240.62796020507812 854.6891479492188 486.5045471191406]\n",
      "[223.78013610839844 239.18914794921875 405.9502258300781 461.126220703125]\n",
      "[1006.7545776367188 101.91900634765625 1121.93359375 236.71133422851562]\n",
      "[652.4445190429688 240.82354736328125 846.8778686523438 491.35931396484375]\n",
      "[224.91371154785156 238.76605224609375 407.5829772949219 461.8937683105469]\n",
      "[657.0046997070312 240.74903869628906 858.2183227539062 490.1163024902344]\n",
      "[225.4509735107422 239.22943115234375 407.78302001953125 461.2403259277344]\n",
      "[656.3372802734375 238.781005859375 860.678955078125 493.4434509277344]\n",
      "[224.73724365234375 239.1863555908203 408.711669921875 462.97064208984375]\n",
      "[656.6331176757812 238.3464813232422 858.3499145507812 490.8133850097656]\n",
      "[225.8094482421875 239.48947143554688 408.65167236328125 463.1777038574219]\n",
      "[1007.0562133789062 99.69011688232422 1117.1729736328125\n",
      " 231.55291748046875]\n",
      "[654.5007934570312 241.08197021484375 854.8086547851562 491.9713439941406]\n",
      "[225.64381408691406 239.87307739257812 408.4651794433594 462.7307434082031]\n",
      "[658.1415405273438 242.7299346923828 854.72509765625 493.03143310546875]\n",
      "[227.50790405273438 239.4658966064453 412.0237731933594 464.138916015625]\n",
      "[1004.6307373046875 99.85873413085938 1116.9176025390625 235.6562042236328]\n",
      "[656.7965698242188 242.88092041015625 852.0391235351562 489.0758056640625]\n",
      "[228.1344451904297 240.56942749023438 414.3464660644531 462.8974914550781]\n",
      "[1004.24365234375 100.59635162353516 1116.69482421875 234.87615966796875]\n",
      "[659.312744140625 240.9239501953125 861.0806884765625 497.9658203125]\n",
      "[228.6696014404297 237.63133239746094 414.2471618652344 462.3537292480469]\n",
      "[1003.9036254882812 97.10722351074219 1117.828857421875 231.61566162109375]\n",
      "[660.596435546875 239.77166748046875 859.506103515625 490.45123291015625]\n",
      "[228.86740112304688 238.15887451171875 408.92333984375 458.3423156738281]\n",
      "[1004.2881469726562 97.23103332519531 1116.8511962890625\n",
      " 228.25155639648438]\n",
      "[660.3013305664062 241.9223175048828 859.2108154296875 493.0616455078125]\n",
      "[229.93882751464844 238.99691772460938 409.87939453125 458.7698974609375]\n",
      "[1003.7470703125 97.93492889404297 1118.2542724609375 228.8531494140625]\n",
      "[660.3504638671875 240.6604461669922 859.67529296875 493.03033447265625]\n",
      "[230.6978302001953 239.8443603515625 410.4833984375 459.2921142578125]\n",
      "[1002.5827026367188 98.54236602783203 1118.1513671875 231.82537841796875]\n",
      "[658.508056640625 240.07559204101562 857.9834594726562 493.5753173828125]\n",
      "[231.29776000976562 239.44793701171875 411.43060302734375\n",
      " 460.35528564453125]\n",
      "[1004.9285278320312 100.1054458618164 1117.2177734375 234.19432067871094]\n",
      "[659.2240600585938 241.57223510742188 859.1244506835938 494.24237060546875]\n",
      "[231.83251953125 238.54898071289062 411.5293884277344 459.3842468261719]\n",
      "[1002.2507934570312 97.05068969726562 1113.00830078125 232.43971252441406]\n",
      "[657.3948364257812 240.59490966796875 857.7003784179688 494.8764953613281]\n",
      "[231.03651428222656 238.7589111328125 410.78289794921875 460.1117858886719]\n",
      "[1001.4795532226562 97.4922866821289 1118.1640625 234.75030517578125]\n",
      "[659.0193481445312 241.38694763183594 858.7312622070312 495.58941650390625]\n",
      "[230.8192596435547 239.4756317138672 411.7752685546875 461.1746826171875]\n",
      "[1002.7094116210938 100.56348419189453 1114.845947265625\n",
      " 235.74098205566406]\n",
      "[660.0692138671875 242.69081115722656 857.7296752929688 494.9615478515625]\n",
      "[232.2058868408203 239.50833129882812 412.16741943359375 460.294189453125]\n",
      "[1001.833251953125 99.92877960205078 1114.5509033203125 236.55747985839844]\n",
      "[657.16943359375 237.15753173828125 859.4559936523438 495.5101013183594]\n",
      "[232.99327087402344 236.90074157714844 410.88677978515625\n",
      " 455.0717468261719]\n",
      "[1000.7802734375 97.99591064453125 1114.5999755859375 236.31944274902344]\n",
      "[659.9291381835938 233.6443634033203 861.1682739257812 491.0722351074219]\n",
      "[232.87196350097656 236.51919555664062 411.2855529785156\n",
      " 455.32354736328125]\n",
      "[999.9629516601562 97.16368103027344 1112.6258544921875 234.88092041015625]\n",
      "[659.6317138671875 242.310302734375 857.8403930664062 496.3061218261719]\n",
      "[231.97463989257812 238.94833374023438 413.02728271484375 459.6396484375]\n",
      "[997.951171875 99.52619171142578 1109.71142578125 234.601806640625]\n",
      "[659.51318359375 241.01919555664062 857.7274169921875 493.45196533203125]\n",
      "[232.74533081054688 238.8472900390625 414.0478210449219 459.8595275878906]\n",
      "[997.38037109375 100.41961669921875 1108.7978515625 234.78927612304688]\n",
      "[659.1695556640625 240.4590301513672 857.0394897460938 492.5270690917969]\n",
      "[234.29483032226562 237.5626983642578 413.18463134765625 454.825927734375]\n",
      "[654.5448608398438 238.18746948242188 856.5214233398438 494.37493896484375]\n",
      "[235.15599060058594 237.68898010253906 414.0851135253906\n",
      " 455.20123291015625]\n",
      "[655.5763549804688 235.88845825195312 858.2745361328125 492.9250793457031]\n",
      "[236.87965393066406 239.08078002929688 414.4792175292969 456.2616882324219]\n",
      "[996.8748779296875 102.57422637939453 1106.02734375 232.43226623535156]\n",
      "[662.0213623046875 244.72674560546875 855.7828979492188 491.9711608886719]\n",
      "[235.08969116210938 238.007080078125 419.6696472167969 462.8695373535156]\n",
      "[996.3265380859375 102.21241760253906 1106.3426513671875\n",
      " 234.96160888671875]\n",
      "[496.72100830078125 196.88877868652344 566.8450317382812\n",
      " 281.72369384765625]\n",
      "[660.2835693359375 244.49856567382812 860.4818725585938 494.20086669921875]\n",
      "[236.4849090576172 238.51138305664062 418.48193359375 460.6572570800781]\n",
      "[995.204833984375 102.6424560546875 1106.83203125 235.1797637939453]\n",
      "[491.224365234375 193.17506408691406 565.7046508789062 281.30926513671875]\n",
      "[654.0701904296875 233.6814727783203 862.5977783203125 494.90911865234375]\n",
      "[236.10398864746094 235.69268798828125 422.3926086425781 467.1626281738281]\n",
      "[996.6187744140625 100.66217803955078 1108.1309814453125\n",
      " 234.45574951171875]\n",
      "[660.6826782226562 238.98753356933594 861.3101806640625 495.34649658203125]\n",
      "[236.5776824951172 237.02001953125 422.9653625488281 469.4496765136719]\n",
      "[995.9583129882812 100.84184265136719 1108.2080078125 235.8916015625]\n",
      "[492.65289306640625 190.21417236328125 569.5790405273438\n",
      " 282.99664306640625]\n",
      "[661.4352416992188 242.6981201171875 857.7532348632812 493.46429443359375]\n",
      "[238.1999969482422 237.57273864746094 421.5560607910156 467.1283264160156]\n",
      "[996.459228515625 101.41961669921875 1107.5682373046875 236.02076721191406]\n",
      "[492.31402587890625 189.94276428222656 571.71875 283.9879455566406]\n",
      "[660.1809692382812 241.43406677246094 857.7489013671875 492.9863586425781]\n",
      "[238.50120544433594 239.70037841796875 421.9359130859375 468.4856872558594]\n",
      "[489.9158020019531 283.40911865234375 654.233154296875 494.6033020019531]\n",
      "[996.3995361328125 102.62899780273438 1107.007568359375 235.24935913085938]\n",
      "[495.92437744140625 193.00584411621094 572.8502197265625 283.9960632324219]\n",
      "[660.453857421875 242.2635040283203 857.3822021484375 493.2440490722656]\n",
      "[237.3635711669922 239.89187622070312 422.3448486328125 471.6476135253906]\n",
      "[995.60107421875 101.2712631225586 1107.3348388671875 234.33248901367188]\n",
      "[495.114013671875 191.06532287597656 571.7171020507812 282.7333068847656]\n",
      "[658.324951171875 239.4244384765625 856.829833984375 493.1601257324219]\n",
      "[238.17665100097656 246.1766815185547 424.2278747558594 476.4418029785156]\n",
      "[495.2443542480469 192.86349487304688 573.28662109375 283.9952392578125]\n",
      "[660.2777709960938 243.69888305664062 855.4075927734375 494.1199951171875]\n",
      "[234.79046630859375 249.8425750732422 425.8691101074219 490.356689453125]\n",
      "[495.6899108886719 194.11744689941406 571.9915161132812 284.18438720703125]\n",
      "[656.714111328125 242.2371368408203 854.2049560546875 495.9654541015625]\n",
      "[234.19102478027344 256.9449768066406 422.1810302734375 493.4631042480469]\n",
      "[494.284912109375 193.88796997070312 570.3892822265625 284.53179931640625]\n",
      "[656.5701293945312 239.80709838867188 854.2575073242188 496.2238464355469]\n",
      "[232.8755340576172 259.11444091796875 423.7301940917969 498.0186767578125]\n",
      "[494.8074951171875 193.2691650390625 571.749755859375 284.3020324707031]\n",
      "[653.35693359375 241.080078125 856.0543823242188 496.1723327636719]\n",
      "[230.3653564453125 260.37359619140625 423.56951904296875 503.9606018066406]\n",
      "[494.8868103027344 192.11830139160156 570.7886352539062 282.16510009765625]\n",
      "[650.3844604492188 238.81866455078125 851.4468383789062 494.68890380859375]\n",
      "[227.50189208984375 257.58941650390625 422.9039306640625 507.900634765625]\n",
      "[494.1789855957031 192.04226684570312 569.3090209960938 281.3980407714844]\n",
      "[650.0896606445312 236.57000732421875 853.3815307617188 493.2186279296875]\n",
      "[227.24343872070312 258.8956604003906 422.215087890625 505.67767333984375]\n",
      "[490.4415283203125 189.8621826171875 568.651611328125 281.44415283203125]\n",
      "[651.0238037109375 239.52047729492188 849.8097534179688 492.3667907714844]\n",
      "[228.0189666748047 263.9734802246094 416.33795166015625 501.50836181640625]\n",
      "[489.8905334472656 188.80184936523438 566.8136596679688 279.8706970214844]\n",
      "[646.8350219726562 237.3670654296875 850.154052734375 491.57537841796875]\n",
      "[227.86038208007812 263.6919860839844 415.4398498535156 501.1529541015625]\n",
      "[486.0918884277344 188.5572509765625 563.44580078125 279.50177001953125]\n",
      "[649.5474853515625 240.85655212402344 851.235595703125 492.21044921875]\n",
      "[226.1916961669922 261.4765930175781 413.477294921875 497.4739074707031]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# cap = cv2.VideoCapture(0)\n",
    "cap = cv2.VideoCapture(\"C:\\\\Users\\\\ADMIN\\\\Documents\\\\GitHub\\\\FaceRecognition-empty\\\\with pretrained model\\\\data\\\\vid\\\\Team.mp4\")\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH,WIDTH)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT,HEIGHT)\n",
    "while cap.isOpened():\n",
    "    isSuccess, frame = cap.read()\n",
    "    if isSuccess==False : break\n",
    "    if isSuccess:\n",
    "        start_time = time.time()\n",
    "        boxes, _ = mtcnn.detect(frame)\n",
    "        end_time = time.time()\n",
    "        if boxes is not None:\n",
    "            for box in boxes:\n",
    "                print(box)\n",
    "                bbox = list(map(int,box.tolist()))\n",
    "                frame = cv2.rectangle(frame,(bbox[0],bbox[1]),(bbox[2],bbox[3]),(0,0,255),6)\n",
    "                \n",
    "                frame = cv2.putText(frame,str(int(1/(end_time-start_time))),(0, 25),cv2.FONT_HERSHEY_SIMPLEX, 1,(0, 255, 0),2,cv2.LINE_AA, False)\n",
    "                cv2.putText(frame, str(bbox), (bbox[0],bbox[1]), cv2.FONT_HERSHEY_DUPLEX, 1, (100, 255, 0), 3, cv2.LINE_AA)\n",
    "                time.sleep(0.0000000000001)\n",
    "    cv2.imshow('Face Detection', frame)\n",
    "    if cv2.waitKey(1)&0xFF == 27:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capture some frames to prepare for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_PATH = r'./data/img/'\n",
    "VID_SRC = r'./data/vid/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_name = input(\"Input ur name: \")\n",
    "if(usr_name!=''):\n",
    "    USR_PATH = os.path.join(IMG_PATH, usr_name)\n",
    "    Source = input(\"Source: \")\n",
    "    if(Source=='cam'):\n",
    "        cap = cv2.VideoCapture(0)\n",
    "    else : \n",
    "        VID_PATH = os.path.join(VID_SRC, usr_name+'.mp4')\n",
    "        cap = cv2.VideoCapture(VID_PATH)\n",
    "    if os.path.exists(USR_PATH):\n",
    "        shutil.rmtree(USR_PATH)\n",
    "    os.makedirs(USR_PATH)\n",
    "    from datetime import datetime\n",
    "\n",
    "    mtcnn = MTCNN(margin = 20,min_face_size=80, thresholds= [0.7, 0.7, 0.8],keep_all=False, select_largest = True, post_process=False, device = device)\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH,WIDTH)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT,HEIGHT)\n",
    "    while cap.isOpened():\n",
    "        isSuccess, frame = cap.read()\n",
    "        if(isSuccess ==False):break\n",
    "        if isSuccess:\n",
    "            boxes, _ = mtcnn.detect(frame)\n",
    "            if boxes is not None:\n",
    "                for box in boxes:\n",
    "                        bbox = list(map(int,box.tolist()))\n",
    "                        dst_points = np.float32([[bbox[0], bbox[1]], [bbox[2], bbox[1]],  [bbox[2], bbox[3]],[bbox[0], bbox[3]]])\n",
    "                        src_points = np.float32([[0, 0],             [WIDTH, 0],          [WIDTH, HEIGHT],   [0, HEIGHT]])\n",
    "                        perspective_matrix = cv2.getPerspectiveTransform(dst_points,src_points)\n",
    "                        warped_image = cv2.warpPerspective(frame, perspective_matrix, (WIDTH, HEIGHT))\n",
    "                        path = str(USR_PATH+'/{}.jpg'.format(str(datetime.now())[:-7].replace(\":\",\"-\").replace(\" \",\"-\")+str(random.randint(1, 999))))\n",
    "                        frame = cv2.rectangle(frame,(bbox[0],bbox[1]),(bbox[2],bbox[3]),(0,0,255))# bbox[0]:top_left_x,bbox[1]:top_left_y,bbox[2]:down_right_x,bbox[3]:down_right_y\n",
    "                        cv2.imwrite(path, warped_image)\n",
    "        cv2.imshow('Face Detection', frame)\n",
    "        if cv2.waitKey(1)&0xFF == 27:\n",
    "            break\n",
    "        \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "IMG_PATH = './data/img'\n",
    "DATA_PATH = './data'\n",
    "embeddings = []\n",
    "names = []\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans(img):\n",
    "    transform = transforms.Compose([\n",
    "            np.float32, \n",
    "            transforms.ToTensor(),\n",
    "            fixed_image_standardization\n",
    "        ])\n",
    "    return transform(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_image_standardization(image_tensor):\n",
    "    processed_tensor = (image_tensor - 127.5) / 128.0\n",
    "    return processed_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModifiedModel(\n",
       "  (model): InceptionResnetV1(\n",
       "    (conv2d_1a): BasicConv2d(\n",
       "      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (conv2d_2a): BasicConv2d(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (conv2d_2b): BasicConv2d(\n",
       "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (maxpool_3a): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv2d_3b): BasicConv2d(\n",
       "      (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (conv2d_4a): BasicConv2d(\n",
       "      (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (conv2d_4b): BasicConv2d(\n",
       "      (conv): Conv2d(192, 256, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (repeat_1): Sequential(\n",
       "      (0): Block35(\n",
       "        (branch0): BasicConv2d(\n",
       "          (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (branch1): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (branch2): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (2): BasicConv2d(\n",
       "            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (conv2d): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (1): Block35(\n",
       "        (branch0): BasicConv2d(\n",
       "          (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (branch1): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (branch2): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (2): BasicConv2d(\n",
       "            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (conv2d): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (2): Block35(\n",
       "        (branch0): BasicConv2d(\n",
       "          (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (branch1): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (branch2): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (2): BasicConv2d(\n",
       "            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (conv2d): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (3): Block35(\n",
       "        (branch0): BasicConv2d(\n",
       "          (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (branch1): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (branch2): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (2): BasicConv2d(\n",
       "            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (conv2d): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (4): Block35(\n",
       "        (branch0): BasicConv2d(\n",
       "          (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (branch1): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (branch2): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (2): BasicConv2d(\n",
       "            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (conv2d): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (mixed_6a): Mixed_6a(\n",
       "      (branch0): BasicConv2d(\n",
       "        (conv): Conv2d(256, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (branch1): Sequential(\n",
       "        (0): BasicConv2d(\n",
       "          (conv): Conv2d(256, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (1): BasicConv2d(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (2): BasicConv2d(\n",
       "          (conv): Conv2d(192, 256, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (branch2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (repeat_2): Sequential(\n",
       "      (0): Block17(\n",
       "        (branch0): BasicConv2d(\n",
       "          (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (branch1): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (2): BasicConv2d(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (conv2d): Conv2d(256, 896, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (1): Block17(\n",
       "        (branch0): BasicConv2d(\n",
       "          (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (branch1): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (2): BasicConv2d(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (conv2d): Conv2d(256, 896, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (2): Block17(\n",
       "        (branch0): BasicConv2d(\n",
       "          (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (branch1): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (2): BasicConv2d(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (conv2d): Conv2d(256, 896, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (3): Block17(\n",
       "        (branch0): BasicConv2d(\n",
       "          (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (branch1): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (2): BasicConv2d(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (conv2d): Conv2d(256, 896, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (4): Block17(\n",
       "        (branch0): BasicConv2d(\n",
       "          (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (branch1): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (2): BasicConv2d(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (conv2d): Conv2d(256, 896, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (5): Block17(\n",
       "        (branch0): BasicConv2d(\n",
       "          (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (branch1): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (2): BasicConv2d(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (conv2d): Conv2d(256, 896, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (6): Block17(\n",
       "        (branch0): BasicConv2d(\n",
       "          (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (branch1): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (2): BasicConv2d(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (conv2d): Conv2d(256, 896, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (7): Block17(\n",
       "        (branch0): BasicConv2d(\n",
       "          (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (branch1): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (2): BasicConv2d(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (conv2d): Conv2d(256, 896, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (8): Block17(\n",
       "        (branch0): BasicConv2d(\n",
       "          (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (branch1): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (2): BasicConv2d(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (conv2d): Conv2d(256, 896, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (9): Block17(\n",
       "        (branch0): BasicConv2d(\n",
       "          (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (branch1): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (2): BasicConv2d(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (conv2d): Conv2d(256, 896, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (mixed_7a): Mixed_7a(\n",
       "      (branch0): Sequential(\n",
       "        (0): BasicConv2d(\n",
       "          (conv): Conv2d(896, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (1): BasicConv2d(\n",
       "          (conv): Conv2d(256, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (branch1): Sequential(\n",
       "        (0): BasicConv2d(\n",
       "          (conv): Conv2d(896, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (1): BasicConv2d(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (branch2): Sequential(\n",
       "        (0): BasicConv2d(\n",
       "          (conv): Conv2d(896, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (1): BasicConv2d(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (2): BasicConv2d(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (branch3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (repeat_3): Sequential(\n",
       "      (0): Block8(\n",
       "        (branch0): BasicConv2d(\n",
       "          (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (branch1): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (2): BasicConv2d(\n",
       "            (conv): Conv2d(192, 192, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (conv2d): Conv2d(384, 1792, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (1): Block8(\n",
       "        (branch0): BasicConv2d(\n",
       "          (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (branch1): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (2): BasicConv2d(\n",
       "            (conv): Conv2d(192, 192, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (conv2d): Conv2d(384, 1792, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (2): Block8(\n",
       "        (branch0): BasicConv2d(\n",
       "          (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (branch1): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (2): BasicConv2d(\n",
       "            (conv): Conv2d(192, 192, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (conv2d): Conv2d(384, 1792, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (3): Block8(\n",
       "        (branch0): BasicConv2d(\n",
       "          (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (branch1): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (2): BasicConv2d(\n",
       "            (conv): Conv2d(192, 192, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (conv2d): Conv2d(384, 1792, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (4): Block8(\n",
       "        (branch0): BasicConv2d(\n",
       "          (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (branch1): Sequential(\n",
       "          (0): BasicConv2d(\n",
       "            (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (1): BasicConv2d(\n",
       "            (conv): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (2): BasicConv2d(\n",
       "            (conv): Conv2d(192, 192, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (conv2d): Conv2d(384, 1792, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (block8): Block8(\n",
       "      (branch0): BasicConv2d(\n",
       "        (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (branch1): Sequential(\n",
       "        (0): BasicConv2d(\n",
       "          (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (1): BasicConv2d(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (2): BasicConv2d(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (conv2d): Conv2d(384, 1792, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (avgpool_1a): AdaptiveAvgPool2d(output_size=1)\n",
       "    (dropout): Dropout(p=0.9, inplace=False)\n",
       "    (last_linear): Linear(in_features=1792, out_features=512, bias=False)\n",
       "    (last_bn): BatchNorm1d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (logits): Linear(in_features=512, out_features=8631, bias=True)\n",
       "  )\n",
       "  (softmax_layer): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "class ModifiedModel(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(ModifiedModel, self).__init__()\n",
    "        self.model = model  # Load the pre-trained model\n",
    "\n",
    "        # Add a new Softmax layer with dim=4\n",
    "        self.softmax_layer = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)  # Pass the input through the pre-trained model\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        output = self.softmax_layer(x)\n",
    "        return output\n",
    "model=ModifiedModel(InceptionResnetV1(\n",
    "    classify=False,\n",
    "    pretrained=\"vggface2\",dropout_prob=0.9).to(device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "names = []\n",
    "for usr in os.listdir(IMG_PATH):\n",
    "    embeds = []\n",
    "    for file in glob.glob(os.path.join(IMG_PATH, usr)+'/*.jpg'):\n",
    "        try:\n",
    "            img = Image.open(file)\n",
    "        except:\n",
    "            continue\n",
    "        with torch.no_grad():\n",
    "            embeds.append(model(trans(img).to(device).unsqueeze(0))) #1 anh, kich thuoc [1,512]\n",
    "    if len(embeds) == 0:\n",
    "        continue\n",
    "    embedding = torch.cat(embeds).mean(0, keepdim=True) #dua ra trung binh cua 50 anh, kich thuoc [1,512]\n",
    "    embeddings.append(embedding) # 1 cai list n cai [1,512]\n",
    "    names.append(usr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = torch.cat(list(embeddings)) #[n,512]\n",
    "names = np.array(names)\n",
    "torch.save(embeddings, DATA_PATH+\"/faceslist.pth\")\n",
    "np.save(DATA_PATH+\"/usernames\", names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0021156403236091137,\n",
       "  0.001921648858115077,\n",
       "  0.001883369404822588,\n",
       "  0.0020845846738666296,\n",
       "  0.0018200103659182787,\n",
       "  0.001800074940547347,\n",
       "  0.0018278133356943727,\n",
       "  0.002048791153356433,\n",
       "  0.0019910777918994427,\n",
       "  0.0019976207986474037,\n",
       "  0.0019901159685105085,\n",
       "  0.0020258885342627764,\n",
       "  0.0018021631985902786,\n",
       "  0.0018853263463824987,\n",
       "  0.001988970674574375,\n",
       "  0.001849622931331396,\n",
       "  0.0019123669480904937,\n",
       "  0.0021085513290017843,\n",
       "  0.0019564004614949226,\n",
       "  0.002084001898765564,\n",
       "  0.001960485242307186,\n",
       "  0.002018261468037963,\n",
       "  0.0018985597416758537,\n",
       "  0.001798245357349515,\n",
       "  0.001770803239196539,\n",
       "  0.001929844031110406,\n",
       "  0.0019483949290588498,\n",
       "  0.0019778553396463394,\n",
       "  0.0019088583067059517,\n",
       "  0.002047198126092553,\n",
       "  0.0020361263304948807,\n",
       "  0.001955895684659481,\n",
       "  0.002036493504419923,\n",
       "  0.0019731379579752684,\n",
       "  0.0017911304021254182,\n",
       "  0.00186274538282305,\n",
       "  0.0018824503058567643,\n",
       "  0.0020648513454943895,\n",
       "  0.002092694863677025,\n",
       "  0.0020804088562726974,\n",
       "  0.0019241550471633673,\n",
       "  0.001968505559489131,\n",
       "  0.0018488974310457706,\n",
       "  0.0020326166413724422,\n",
       "  0.0019111724104732275,\n",
       "  0.0019794469699263573,\n",
       "  0.0018418858526274562,\n",
       "  0.0020112053025513887,\n",
       "  0.0018232513684779406,\n",
       "  0.001966036856174469,\n",
       "  0.0019436771981418133,\n",
       "  0.0019084903178736567,\n",
       "  0.001980418339371681,\n",
       "  0.0019937872420996428,\n",
       "  0.0018251246074214578,\n",
       "  0.0019006567308679223,\n",
       "  0.002028857357800007,\n",
       "  0.001947286888025701,\n",
       "  0.0018615659791976213,\n",
       "  0.001876276102848351,\n",
       "  0.0017130953492596745,\n",
       "  0.002150041051208973,\n",
       "  0.0018466474721208215,\n",
       "  0.001962654758244753,\n",
       "  0.002005727728828788,\n",
       "  0.0019253771752119064,\n",
       "  0.001954031875357032,\n",
       "  0.0018855803646147251,\n",
       "  0.0019000552129000425,\n",
       "  0.0018115382408723235,\n",
       "  0.0020959333050996065,\n",
       "  0.002055973978713155,\n",
       "  0.0018237738404422998,\n",
       "  0.0018473564414307475,\n",
       "  0.001954204635694623,\n",
       "  0.0018303534016013145,\n",
       "  0.0018747483845800161,\n",
       "  0.0019181055249646306,\n",
       "  0.0018689102726057172,\n",
       "  0.001866987207904458,\n",
       "  0.001975563820451498,\n",
       "  0.001960877561941743,\n",
       "  0.002056447323411703,\n",
       "  0.0018713035387918353,\n",
       "  0.0019347957568243146,\n",
       "  0.0019483050564303994,\n",
       "  0.0019238204695284367,\n",
       "  0.00188397616147995,\n",
       "  0.0018247866537421942,\n",
       "  0.0019282104913145304,\n",
       "  0.001998711610212922,\n",
       "  0.0019097174517810345,\n",
       "  0.001864670543000102,\n",
       "  0.00181157689075917,\n",
       "  0.00195647357031703,\n",
       "  0.0020313668064773083,\n",
       "  0.0020373330917209387,\n",
       "  0.0019290001364424825,\n",
       "  0.0017546345479786396,\n",
       "  0.0020852130837738514,\n",
       "  0.0019378906581550837,\n",
       "  0.0020729447714984417,\n",
       "  0.0019566670525819063,\n",
       "  0.0020307861268520355,\n",
       "  0.002052401192486286,\n",
       "  0.0019395225681364536,\n",
       "  0.0020022832322865725,\n",
       "  0.0020554831717163324,\n",
       "  0.001975474413484335,\n",
       "  0.002006059279665351,\n",
       "  0.002177903661504388,\n",
       "  0.0019550493452697992,\n",
       "  0.001897027948871255,\n",
       "  0.001976801548153162,\n",
       "  0.002021654974669218,\n",
       "  0.002073279581964016,\n",
       "  0.0019998373463749886,\n",
       "  0.002027482958510518,\n",
       "  0.0019969078712165356,\n",
       "  0.002058581914752722,\n",
       "  0.0018638386391103268,\n",
       "  0.002025172347202897,\n",
       "  0.0019373097456991673,\n",
       "  0.001886417274363339,\n",
       "  0.0018450351199135184,\n",
       "  0.0018834994407370687,\n",
       "  0.0019083035876974463,\n",
       "  0.001847314415499568,\n",
       "  0.001853070454671979,\n",
       "  0.0018129765521734953,\n",
       "  0.0018937542336061597,\n",
       "  0.0018761585233733058,\n",
       "  0.0019673884380608797,\n",
       "  0.001957374857738614,\n",
       "  0.001962417270988226,\n",
       "  0.002077715704217553,\n",
       "  0.001877482864074409,\n",
       "  0.0019171610474586487,\n",
       "  0.002034873701632023,\n",
       "  0.0019795752596110106,\n",
       "  0.0019437726587057114,\n",
       "  0.0020705678034573793,\n",
       "  0.0018416877137497067,\n",
       "  0.002082336926832795,\n",
       "  0.0020733382552862167,\n",
       "  0.00205603358335793,\n",
       "  0.0019582973327487707,\n",
       "  0.0020103375427424908,\n",
       "  0.0018549698870629072,\n",
       "  0.001980044413357973,\n",
       "  0.001953938277438283,\n",
       "  0.0020166123285889626,\n",
       "  0.0018694143509492278,\n",
       "  0.0018249076092615724,\n",
       "  0.002165988553315401,\n",
       "  0.0020852077286690474,\n",
       "  0.0019365925109013915,\n",
       "  0.0020262710750102997,\n",
       "  0.0018564651254564524,\n",
       "  0.0019349640933796763,\n",
       "  0.001956010004505515,\n",
       "  0.0020291227847337723,\n",
       "  0.0018982063047587872,\n",
       "  0.002040944527834654,\n",
       "  0.0019148255232721567,\n",
       "  0.0021046854089945555,\n",
       "  0.0018596276640892029,\n",
       "  0.0020037805661559105,\n",
       "  0.0017757555469870567,\n",
       "  0.002026801463216543,\n",
       "  0.0018652810249477625,\n",
       "  0.001915619010105729,\n",
       "  0.001983284018933773,\n",
       "  0.002094278112053871,\n",
       "  0.0020616985857486725,\n",
       "  0.002025290159508586,\n",
       "  0.001937537919729948,\n",
       "  0.0018196783494204283,\n",
       "  0.0019308226183056831,\n",
       "  0.0019197670044377446,\n",
       "  0.002023062203079462,\n",
       "  0.0018332527251914144,\n",
       "  0.0020607111509889364,\n",
       "  0.0019259414402768016,\n",
       "  0.002044019289314747,\n",
       "  0.0019414739217609167,\n",
       "  0.00201769289560616,\n",
       "  0.002004783833399415,\n",
       "  0.002146159764379263,\n",
       "  0.0019161300733685493,\n",
       "  0.002000730950385332,\n",
       "  0.0018761638784781098,\n",
       "  0.002071755938231945,\n",
       "  0.002004977548494935,\n",
       "  0.002126418286934495,\n",
       "  0.0020038329530507326,\n",
       "  0.0018479841528460383,\n",
       "  0.002071849536150694,\n",
       "  0.0020043912809342146,\n",
       "  0.001986051443964243,\n",
       "  0.0019523167284205556,\n",
       "  0.0019111407455056906,\n",
       "  0.001959379529580474,\n",
       "  0.0019449146930128336,\n",
       "  0.002087582368403673,\n",
       "  0.001982571557164192,\n",
       "  0.002104034647345543,\n",
       "  0.001995715545490384,\n",
       "  0.0020455252379179,\n",
       "  0.001995060360059142,\n",
       "  0.001979218330234289,\n",
       "  0.002038998296484351,\n",
       "  0.0019446684746071696,\n",
       "  0.0019730853382498026,\n",
       "  0.0019526624819263816,\n",
       "  0.00184200971852988,\n",
       "  0.002045001834630966,\n",
       "  0.0019496086752042174,\n",
       "  0.0020402923692017794,\n",
       "  0.0019182040123268962,\n",
       "  0.001997873419895768,\n",
       "  0.0019639646634459496,\n",
       "  0.0019794772379100323,\n",
       "  0.0019523854134604335,\n",
       "  0.0019494123989716172,\n",
       "  0.0018568516243249178,\n",
       "  0.0020049510058015585,\n",
       "  0.0019480064511299133,\n",
       "  0.0018436404643580317,\n",
       "  0.0019998664502054453,\n",
       "  0.0019366005435585976,\n",
       "  0.0020468621514737606,\n",
       "  0.002014739206060767,\n",
       "  0.0018409062176942825,\n",
       "  0.0018122363835573196,\n",
       "  0.001978487940505147,\n",
       "  0.001914944383315742,\n",
       "  0.0021477178670465946,\n",
       "  0.001900099916383624,\n",
       "  0.0018910180078819394,\n",
       "  0.0018833098001778126,\n",
       "  0.0019280898850411177,\n",
       "  0.0020108213648200035,\n",
       "  0.002024509012699127,\n",
       "  0.0019617490470409393,\n",
       "  0.0020958709064871073,\n",
       "  0.0020180484279990196,\n",
       "  0.001947528449818492,\n",
       "  0.0019401268800720572,\n",
       "  0.001982829300686717,\n",
       "  0.0018545098137110472,\n",
       "  0.0019137430936098099,\n",
       "  0.0021158410236239433,\n",
       "  0.0020250503439456224,\n",
       "  0.001910452265292406,\n",
       "  0.0019616715144366026,\n",
       "  0.001994978403672576,\n",
       "  0.001905855955556035,\n",
       "  0.0019826393108814955,\n",
       "  0.002016511047258973,\n",
       "  0.001928875339217484,\n",
       "  0.001985437236726284,\n",
       "  0.0018786791479215026,\n",
       "  0.001872229971922934,\n",
       "  0.0018710145959630609,\n",
       "  0.001957333879545331,\n",
       "  0.0019836705178022385,\n",
       "  0.0018433125223964453,\n",
       "  0.0017971809720620513,\n",
       "  0.0018726479029282928,\n",
       "  0.001903824508190155,\n",
       "  0.0019754068925976753,\n",
       "  0.0019989560823887587,\n",
       "  0.0018892385996878147,\n",
       "  0.0019275241065770388,\n",
       "  0.002051939722150564,\n",
       "  0.0019494190346449614,\n",
       "  0.0018135705031454563,\n",
       "  0.002001446206122637,\n",
       "  0.0019554137252271175,\n",
       "  0.0018954300321638584,\n",
       "  0.0020327274687588215,\n",
       "  0.0019107944099232554,\n",
       "  0.0019257537787780166,\n",
       "  0.0018553417176008224,\n",
       "  0.0020996418315917253,\n",
       "  0.0021237570326775312,\n",
       "  0.0019417870789766312,\n",
       "  0.0019685712177306414,\n",
       "  0.0020061128307133913,\n",
       "  0.001847501378506422,\n",
       "  0.0018203735817223787,\n",
       "  0.0020000196527689695,\n",
       "  0.0020854102913290262,\n",
       "  0.001830327557399869,\n",
       "  0.001726372865960002,\n",
       "  0.001983347348868847,\n",
       "  0.0018895871471613646,\n",
       "  0.0019450755789875984,\n",
       "  0.001988216070458293,\n",
       "  0.0018336751963943243,\n",
       "  0.0018612506100907922,\n",
       "  0.0017855721525847912,\n",
       "  0.001969005214050412,\n",
       "  0.0018623473588377237,\n",
       "  0.0019551466684788465,\n",
       "  0.0018816000083461404,\n",
       "  0.002023197477683425,\n",
       "  0.0019530915888026357,\n",
       "  0.001992646139115095,\n",
       "  0.002017334569245577,\n",
       "  0.0020130302291363478,\n",
       "  0.001954962033778429,\n",
       "  0.002058781683444977,\n",
       "  0.0019256479572504759,\n",
       "  0.0019329821225255728,\n",
       "  0.002020526211708784,\n",
       "  0.0017614542739465833,\n",
       "  0.0020974332001060247,\n",
       "  0.001998012652620673,\n",
       "  0.0019913234282284975,\n",
       "  0.0020045253913849592,\n",
       "  0.0019359345315024257,\n",
       "  0.0020634052343666553,\n",
       "  0.002060127444565296,\n",
       "  0.001877872971817851,\n",
       "  0.002069566398859024,\n",
       "  0.0019688785541802645,\n",
       "  0.001956603256985545,\n",
       "  0.0019540407229214907,\n",
       "  0.0019855170976370573,\n",
       "  0.001959636341780424,\n",
       "  0.0018828752217814326,\n",
       "  0.002025546273216605,\n",
       "  0.0019607283174991608,\n",
       "  0.0020976150408387184,\n",
       "  0.0018616170855239034,\n",
       "  0.0018783417763188481,\n",
       "  0.001921551302075386,\n",
       "  0.0018576179863885045,\n",
       "  0.0019163026008754969,\n",
       "  0.0018844319274649024,\n",
       "  0.0020307910162955523,\n",
       "  0.002021998632699251,\n",
       "  0.0019894109573215246,\n",
       "  0.0018409602344036102,\n",
       "  0.0019771032966673374,\n",
       "  0.001961548114195466,\n",
       "  0.0020023903343826532,\n",
       "  0.001977234613150358,\n",
       "  0.0018287725979462266,\n",
       "  0.0020838764030486345,\n",
       "  0.0018317587673664093,\n",
       "  0.0020045398268848658,\n",
       "  0.001936624408699572,\n",
       "  0.001946645905263722,\n",
       "  0.0019156326306983829,\n",
       "  0.002013934077695012,\n",
       "  0.0019743696320801973,\n",
       "  0.0020140947308391333,\n",
       "  0.0020061086397618055,\n",
       "  0.00200557429343462,\n",
       "  0.0019287554314360023,\n",
       "  0.002045166678726673,\n",
       "  0.0019820178858935833,\n",
       "  0.0020168814808130264,\n",
       "  0.0018998774467036128,\n",
       "  0.002054293639957905,\n",
       "  0.002037942176684737,\n",
       "  0.0019040429033339024,\n",
       "  0.0019316143589094281,\n",
       "  0.0018244856037199497,\n",
       "  0.00198543188162148,\n",
       "  0.0019019779283553362,\n",
       "  0.001855650101788342,\n",
       "  0.0018960977904498577,\n",
       "  0.0019422986079007387,\n",
       "  0.0020269083324819803,\n",
       "  0.001997551415115595,\n",
       "  0.0017925086431205273,\n",
       "  0.0019806367345154285,\n",
       "  0.0018680947832763195,\n",
       "  0.001930320286192,\n",
       "  0.0019893681164830923,\n",
       "  0.0018975804559886456,\n",
       "  0.0018965060589835048,\n",
       "  0.001994912512600422,\n",
       "  0.00188732601236552,\n",
       "  0.0018809741595759988,\n",
       "  0.0019152588210999966,\n",
       "  0.0020754120778292418,\n",
       "  0.0019454702269285917,\n",
       "  0.0019328688504174352,\n",
       "  0.0018317417707294226,\n",
       "  0.0019545815885066986,\n",
       "  0.00198476598598063,\n",
       "  0.0018353720661252737,\n",
       "  0.0019438413437455893,\n",
       "  0.0020355915185064077,\n",
       "  0.0018891810905188322,\n",
       "  0.0021330032031983137,\n",
       "  0.001987123629078269,\n",
       "  0.0019574067555367947,\n",
       "  0.002064639003947377,\n",
       "  0.0020941118709743023,\n",
       "  0.001855860697105527,\n",
       "  0.0018109187949448824,\n",
       "  0.0019055625889450312,\n",
       "  0.0019231003243476152,\n",
       "  0.0019228339660912752,\n",
       "  0.0019393296679481864,\n",
       "  0.001889726030640304,\n",
       "  0.001967620337381959,\n",
       "  0.0019090004498139024,\n",
       "  0.001948599936440587,\n",
       "  0.0019281491404399276,\n",
       "  0.002062795916572213,\n",
       "  0.0019359958823770285,\n",
       "  0.001981981098651886,\n",
       "  0.001824136357754469,\n",
       "  0.0019692860078066587,\n",
       "  0.002045944333076477,\n",
       "  0.001953772734850645,\n",
       "  0.0018915495602414012,\n",
       "  0.0018439359264448285,\n",
       "  0.001842861995100975,\n",
       "  0.001958030043169856,\n",
       "  0.0019367651548236609,\n",
       "  0.0019403750775381923,\n",
       "  0.001979895867407322,\n",
       "  0.001959435874596238,\n",
       "  0.0019517035689204931,\n",
       "  0.0019384744809940457,\n",
       "  0.0018361862748861313,\n",
       "  0.0019169278675690293,\n",
       "  0.0019590763840824366,\n",
       "  0.0018443047301843762,\n",
       "  0.0018547062063589692,\n",
       "  0.001926785334944725,\n",
       "  0.0019328435882925987,\n",
       "  0.0019387356005609035,\n",
       "  0.002022422617301345,\n",
       "  0.0018944452749565244,\n",
       "  0.0020271455869078636,\n",
       "  0.0018433273071423173,\n",
       "  0.0018960346933454275,\n",
       "  0.001984132220968604,\n",
       "  0.0018634701846167445,\n",
       "  0.0017763205105438828,\n",
       "  0.0019122111843898892,\n",
       "  0.001849914202466607,\n",
       "  0.0020215525291860104,\n",
       "  0.0017739306204020977,\n",
       "  0.0021867540199309587,\n",
       "  0.0019462533527985215,\n",
       "  0.0019694147631525993,\n",
       "  0.0019858444575220346,\n",
       "  0.0018926768098026514,\n",
       "  0.002037114230915904,\n",
       "  0.002059496007859707,\n",
       "  0.0021207728423178196,\n",
       "  0.0019688718020915985,\n",
       "  0.0019827692303806543,\n",
       "  0.0017860104562714696,\n",
       "  0.0019081318750977516,\n",
       "  0.0021423944272100925,\n",
       "  0.002019159961491823,\n",
       "  0.0019937397446483374,\n",
       "  0.0019720401614904404,\n",
       "  0.002075379714369774,\n",
       "  0.0021596597507596016,\n",
       "  0.0019002689514309168,\n",
       "  0.002171978587284684,\n",
       "  0.001816515694372356,\n",
       "  0.0019145876867696643,\n",
       "  0.0018585260258987546,\n",
       "  0.0019122163066640496,\n",
       "  0.0019166420679539442,\n",
       "  0.0020651970990002155,\n",
       "  0.0020376136526465416,\n",
       "  0.0019931343849748373,\n",
       "  0.0019024059874936938,\n",
       "  0.0020327302627265453,\n",
       "  0.0020505543798208237,\n",
       "  0.001972222700715065,\n",
       "  0.001986557152122259,\n",
       "  0.0018789898604154587,\n",
       "  0.0019406857900321484,\n",
       "  0.0019973006565123796,\n",
       "  0.001994352787733078,\n",
       "  0.001978126121684909,\n",
       "  0.0018984997877851129,\n",
       "  0.0019274988444522023,\n",
       "  0.0018843309953808784,\n",
       "  0.0019391010282561183,\n",
       "  0.0019167799036949873,\n",
       "  0.001982055837288499,\n",
       "  0.0019041488412767649,\n",
       "  0.0019605427514761686,\n",
       "  0.001880067866295576,\n",
       "  0.0019275288796052337,\n",
       "  0.0020025584381073713,\n",
       "  0.0019539245404303074,\n",
       "  0.002023649401962757,\n",
       "  0.0020252116955816746,\n",
       "  0.0019646675791591406,\n",
       "  0.0018667272524908185,\n",
       "  0.002076471457257867,\n",
       "  0.0018657045438885689,\n",
       "  0.0018551157554611564,\n",
       "  0.0019726071041077375,\n",
       "  0.0018867523176595569],\n",
       " [0.002093178452923894,\n",
       "  0.0019073305884376168,\n",
       "  0.0018863677978515625,\n",
       "  0.002074300078675151,\n",
       "  0.0018436911050230265,\n",
       "  0.001787385786883533,\n",
       "  0.0018028985941782594,\n",
       "  0.0020431485027074814,\n",
       "  0.0020152286160737276,\n",
       "  0.0019363642204552889,\n",
       "  0.001967337680980563,\n",
       "  0.0020361158531159163,\n",
       "  0.0018308296566829085,\n",
       "  0.0019131561275571585,\n",
       "  0.0020057675428688526,\n",
       "  0.0018910738872364163,\n",
       "  0.0019475312437862158,\n",
       "  0.002166193211451173,\n",
       "  0.0019419726449996233,\n",
       "  0.002100474899634719,\n",
       "  0.002007843926548958,\n",
       "  0.001978703308850527,\n",
       "  0.0019024177454411983,\n",
       "  0.001838965225033462,\n",
       "  0.0018128968076780438,\n",
       "  0.0019140379736199975,\n",
       "  0.0019474925938993692,\n",
       "  0.0020030527375638485,\n",
       "  0.0019421388860791922,\n",
       "  0.0020670562516897917,\n",
       "  0.002094794763252139,\n",
       "  0.0019277624087408185,\n",
       "  0.002011509146541357,\n",
       "  0.001991217490285635,\n",
       "  0.0018135333666577935,\n",
       "  0.001884416793473065,\n",
       "  0.0018615785520523787,\n",
       "  0.002040591323748231,\n",
       "  0.002055876422673464,\n",
       "  0.0021731529850512743,\n",
       "  0.0019432274857535958,\n",
       "  0.0019262944115325809,\n",
       "  0.0018439347622916102,\n",
       "  0.00204468029551208,\n",
       "  0.0019160956144332886,\n",
       "  0.0019871436525136232,\n",
       "  0.0018426418537274003,\n",
       "  0.0019987188279628754,\n",
       "  0.0018091609235852957,\n",
       "  0.0019658911041915417,\n",
       "  0.0019675653893500566,\n",
       "  0.0019137236522510648,\n",
       "  0.0019917914178222418,\n",
       "  0.002002063672989607,\n",
       "  0.0018245709361508489,\n",
       "  0.001881730160675943,\n",
       "  0.0020190360955893993,\n",
       "  0.0019152608001604676,\n",
       "  0.0019010527757927775,\n",
       "  0.0018804420251399279,\n",
       "  0.0017227992648258805,\n",
       "  0.0021263519302010536,\n",
       "  0.001838878495618701,\n",
       "  0.0019680012483149767,\n",
       "  0.00197666697204113,\n",
       "  0.0019686713349074125,\n",
       "  0.0019314648816362023,\n",
       "  0.0019095075549557805,\n",
       "  0.0018779507372528315,\n",
       "  0.0017867462011054158,\n",
       "  0.0020955665968358517,\n",
       "  0.0019838802982121706,\n",
       "  0.001835602568462491,\n",
       "  0.001832971815019846,\n",
       "  0.0019817701540887356,\n",
       "  0.0018357118824496865,\n",
       "  0.0019010052783414721,\n",
       "  0.0019605476409196854,\n",
       "  0.0018677586922422051,\n",
       "  0.0018706993432715535,\n",
       "  0.0019611474126577377,\n",
       "  0.0019420081516727805,\n",
       "  0.0020603882148861885,\n",
       "  0.0018914821557700634,\n",
       "  0.0019351844675838947,\n",
       "  0.0019197004148736596,\n",
       "  0.001946001430042088,\n",
       "  0.0019615106284618378,\n",
       "  0.0018212131690233946,\n",
       "  0.0019465177319943905,\n",
       "  0.001981554552912712,\n",
       "  0.0019251287449151278,\n",
       "  0.0018814136274158955,\n",
       "  0.0018374132923781872,\n",
       "  0.0019616028293967247,\n",
       "  0.0019645048305392265,\n",
       "  0.0020032688044011593,\n",
       "  0.0019083945080637932,\n",
       "  0.0017685173079371452,\n",
       "  0.002125311875715852,\n",
       "  0.0019346069311723113,\n",
       "  0.0021293906029313803,\n",
       "  0.0019866893999278545,\n",
       "  0.0020206118933856487,\n",
       "  0.002090202644467354,\n",
       "  0.0019567993003875017,\n",
       "  0.001992578385397792,\n",
       "  0.002040499122813344,\n",
       "  0.0019975556060671806,\n",
       "  0.0020258426666259766,\n",
       "  0.0021413909271359444,\n",
       "  0.001969404285773635,\n",
       "  0.0018737124046310782,\n",
       "  0.0020084623247385025,\n",
       "  0.002045874949544668,\n",
       "  0.002066434361040592,\n",
       "  0.001962859882041812,\n",
       "  0.0020178277045488358,\n",
       "  0.0019224487477913499,\n",
       "  0.002015243051573634,\n",
       "  0.0019171941094100475,\n",
       "  0.0020696844439953566,\n",
       "  0.001889189356006682,\n",
       "  0.0019230656325817108,\n",
       "  0.0018604930955916643,\n",
       "  0.0018768304726108909,\n",
       "  0.001849290682002902,\n",
       "  0.0018733824836090207,\n",
       "  0.001908145030029118,\n",
       "  0.001842794823460281,\n",
       "  0.0019059509504586458,\n",
       "  0.0018924897303804755,\n",
       "  0.00197691866196692,\n",
       "  0.0019386222120374441,\n",
       "  0.0020267986692488194,\n",
       "  0.0020921833347529173,\n",
       "  0.0019095804309472442,\n",
       "  0.0018614267464727163,\n",
       "  0.0020179387647658587,\n",
       "  0.0019413635600358248,\n",
       "  0.00198043929412961,\n",
       "  0.0020223658066242933,\n",
       "  0.0018237297190353274,\n",
       "  0.002074545482173562,\n",
       "  0.002117645228281617,\n",
       "  0.0020165941677987576,\n",
       "  0.0019287571776658297,\n",
       "  0.0020040813833475113,\n",
       "  0.001903077238239348,\n",
       "  0.001923474483191967,\n",
       "  0.0019270539050921798,\n",
       "  0.001998645719140768,\n",
       "  0.0019205904100090265,\n",
       "  0.001856146496720612,\n",
       "  0.0021745585836470127,\n",
       "  0.001992919249460101,\n",
       "  0.0019864949863404036,\n",
       "  0.002006746595725417,\n",
       "  0.0018450289499014616,\n",
       "  0.0019362830789759755,\n",
       "  0.001912684179842472,\n",
       "  0.0020528153982013464,\n",
       "  0.001903102733194828,\n",
       "  0.0020731664262712,\n",
       "  0.0019910219125449657,\n",
       "  0.002132407622411847,\n",
       "  0.001839278033003211,\n",
       "  0.0020288259256631136,\n",
       "  0.001768475049175322,\n",
       "  0.0019695917144417763,\n",
       "  0.001847155042923987,\n",
       "  0.001916356268338859,\n",
       "  0.001967600779607892,\n",
       "  0.0021176321897655725,\n",
       "  0.0020477110520005226,\n",
       "  0.00198132311925292,\n",
       "  0.0019443854689598083,\n",
       "  0.00180399464443326,\n",
       "  0.0018910588696599007,\n",
       "  0.0019392733229324222,\n",
       "  0.0020151499193161726,\n",
       "  0.0018651256104931235,\n",
       "  0.0020382264629006386,\n",
       "  0.0019170540617778897,\n",
       "  0.002044168533757329,\n",
       "  0.0019631406757980585,\n",
       "  0.0019573962781578302,\n",
       "  0.0019521147478371859,\n",
       "  0.0020979999098926783,\n",
       "  0.0019019957398995757,\n",
       "  0.001973530277609825,\n",
       "  0.0018847020110115409,\n",
       "  0.0020873232278972864,\n",
       "  0.0020071249455213547,\n",
       "  0.0021046269685029984,\n",
       "  0.0019975057803094387,\n",
       "  0.0018363066483289003,\n",
       "  0.002082493854686618,\n",
       "  0.002044154331088066,\n",
       "  0.0019769188947975636,\n",
       "  0.0019633860792964697,\n",
       "  0.0018905169563367963,\n",
       "  0.001981897745281458,\n",
       "  0.0019518473418429494,\n",
       "  0.002074228599667549,\n",
       "  0.0019370177760720253,\n",
       "  0.002079747850075364,\n",
       "  0.002035881159827113,\n",
       "  0.0020544559229165316,\n",
       "  0.0020154407247900963,\n",
       "  0.0019622535910457373,\n",
       "  0.0020525017753243446,\n",
       "  0.0019210121827200055,\n",
       "  0.0020350664854049683,\n",
       "  0.0019552158191800117,\n",
       "  0.0018511988455429673,\n",
       "  0.0020152165088802576,\n",
       "  0.001971370540559292,\n",
       "  0.0020373202860355377,\n",
       "  0.001852666144259274,\n",
       "  0.0019993216264992952,\n",
       "  0.001967770280316472,\n",
       "  0.0019895685836672783,\n",
       "  0.0019443267956376076,\n",
       "  0.0019422824261710048,\n",
       "  0.001857534982264042,\n",
       "  0.0019962426740676165,\n",
       "  0.0019533601589500904,\n",
       "  0.0018352471524849534,\n",
       "  0.0020087980665266514,\n",
       "  0.0018745667766779661,\n",
       "  0.0020072285551577806,\n",
       "  0.0020448104478418827,\n",
       "  0.001850269502028823,\n",
       "  0.0018709483556449413,\n",
       "  0.001976183382794261,\n",
       "  0.0018806959269568324,\n",
       "  0.002134493784978986,\n",
       "  0.0019287063041701913,\n",
       "  0.0018831170164048672,\n",
       "  0.0018534362316131592,\n",
       "  0.0019350707298144698,\n",
       "  0.0020867218263447285,\n",
       "  0.002032258315011859,\n",
       "  0.0019812791142612696,\n",
       "  0.002120079006999731,\n",
       "  0.0020220126025378704,\n",
       "  0.0019932815339416265,\n",
       "  0.0019237869419157505,\n",
       "  0.001977589214220643,\n",
       "  0.001888855593279004,\n",
       "  0.0019495512824505568,\n",
       "  0.002109189284965396,\n",
       "  0.0019549524877220392,\n",
       "  0.0019428638042882085,\n",
       "  0.0019483693176880479,\n",
       "  0.0020607993938028812,\n",
       "  0.0018957799766212702,\n",
       "  0.0019532209262251854,\n",
       "  0.0020129603799432516,\n",
       "  0.0019032831769436598,\n",
       "  0.0019499666523188353,\n",
       "  0.001825492363423109,\n",
       "  0.001894560526125133,\n",
       "  0.0018835273804143071,\n",
       "  0.0019089463166892529,\n",
       "  0.0019915876910090446,\n",
       "  0.0018258538329973817,\n",
       "  0.0017676855204626918,\n",
       "  0.0019116158364340663,\n",
       "  0.0018339487724006176,\n",
       "  0.0019131888402625918,\n",
       "  0.001991103868931532,\n",
       "  0.0018640222260728478,\n",
       "  0.0019379560835659504,\n",
       "  0.002036021091043949,\n",
       "  0.0019640224054455757,\n",
       "  0.001806706190109253,\n",
       "  0.0019744301680475473,\n",
       "  0.0019611481111496687,\n",
       "  0.0018949911464005709,\n",
       "  0.0019946007523685694,\n",
       "  0.0019217260414734483,\n",
       "  0.0019929565023630857,\n",
       "  0.0019050088012591004,\n",
       "  0.002101218793541193,\n",
       "  0.0020983582362532616,\n",
       "  0.0018905559554696083,\n",
       "  0.001983195310458541,\n",
       "  0.002008721698075533,\n",
       "  0.0018713357858359814,\n",
       "  0.001860038610175252,\n",
       "  0.002036692574620247,\n",
       "  0.001990352524444461,\n",
       "  0.0018811115296557546,\n",
       "  0.0017898615915328264,\n",
       "  0.002011718926951289,\n",
       "  0.0018963690381497145,\n",
       "  0.0019727901089936495,\n",
       "  0.0020288017112761736,\n",
       "  0.0018631592392921448,\n",
       "  0.0018616945017129183,\n",
       "  0.0017779591726139188,\n",
       "  0.001945552765391767,\n",
       "  0.0018314713379368186,\n",
       "  0.0019570612348616123,\n",
       "  0.0019183182157576084,\n",
       "  0.0019863895140588284,\n",
       "  0.00196825060993433,\n",
       "  0.0019309679046273232,\n",
       "  0.0019565224647521973,\n",
       "  0.0019955900497734547,\n",
       "  0.0019666727166622877,\n",
       "  0.0020252561662346125,\n",
       "  0.001902389107272029,\n",
       "  0.0019413518020883203,\n",
       "  0.0020377174951136112,\n",
       "  0.001745428889989853,\n",
       "  0.002162714023143053,\n",
       "  0.002000871580094099,\n",
       "  0.0019534658640623093,\n",
       "  0.0019722639117389917,\n",
       "  0.0019306516041979194,\n",
       "  0.002040158724412322,\n",
       "  0.002019305946305394,\n",
       "  0.0018963209586218,\n",
       "  0.002051865914836526,\n",
       "  0.0020163212902843952,\n",
       "  0.0019595820922404528,\n",
       "  0.0020008927676826715,\n",
       "  0.0019434713758528233,\n",
       "  0.001952394493855536,\n",
       "  0.0018778262892737985,\n",
       "  0.0019951006397604942,\n",
       "  0.00198347051627934,\n",
       "  0.0020637770649045706,\n",
       "  0.0018813976785168052,\n",
       "  0.0018942521419376135,\n",
       "  0.0018996228463947773,\n",
       "  0.0018814289942383766,\n",
       "  0.0018405297305434942,\n",
       "  0.0018389259930700064,\n",
       "  0.002030732110142708,\n",
       "  0.002049833070486784,\n",
       "  0.0020195734687149525,\n",
       "  0.0018855349626392126,\n",
       "  0.0019996112678200006,\n",
       "  0.001999074360355735,\n",
       "  0.001992209814488888,\n",
       "  0.0019910219125449657,\n",
       "  0.0018888701451942325,\n",
       "  0.0020783140789717436,\n",
       "  0.0018508238717913628,\n",
       "  0.001989934593439102,\n",
       "  0.0019844688940793276,\n",
       "  0.0019336871337145567,\n",
       "  0.0018684613751247525,\n",
       "  0.002008024137467146,\n",
       "  0.0019471731502562761,\n",
       "  0.0020007200073450804,\n",
       "  0.002041118685156107,\n",
       "  0.002019584644585848,\n",
       "  0.001901615527458489,\n",
       "  0.002011379925534129,\n",
       "  0.0020141832064837217,\n",
       "  0.0020028087310492992,\n",
       "  0.0019001176115125418,\n",
       "  0.0021049489732831717,\n",
       "  0.002014470286667347,\n",
       "  0.001848186133429408,\n",
       "  0.0019393591210246086,\n",
       "  0.0017951243789866567,\n",
       "  0.0020319714676588774,\n",
       "  0.0019144307589158416,\n",
       "  0.0018662033835425973,\n",
       "  0.001885095378383994,\n",
       "  0.0019308861810714006,\n",
       "  0.0020338050089776516,\n",
       "  0.001984118018299341,\n",
       "  0.001787684508599341,\n",
       "  0.0019604850094765425,\n",
       "  0.001886931830085814,\n",
       "  0.0018983084009960294,\n",
       "  0.001998846884816885,\n",
       "  0.0018352058250457048,\n",
       "  0.0018832576461136341,\n",
       "  0.0019396855495870113,\n",
       "  0.0018495278200134635,\n",
       "  0.0018972664838656783,\n",
       "  0.0019048593239858747,\n",
       "  0.0020543045829981565,\n",
       "  0.001971988473087549,\n",
       "  0.0019287242321297526,\n",
       "  0.0018631798448041081,\n",
       "  0.001933746738359332,\n",
       "  0.001974777551367879,\n",
       "  0.0018328415462747216,\n",
       "  0.001969460863620043,\n",
       "  0.002026156522333622,\n",
       "  0.0018832051428034902,\n",
       "  0.002153191715478897,\n",
       "  0.00200598849914968,\n",
       "  0.001972850412130356,\n",
       "  0.00201129331253469,\n",
       "  0.002080206060782075,\n",
       "  0.0018773938063532114,\n",
       "  0.001858255360275507,\n",
       "  0.001902079558931291,\n",
       "  0.0019282135181128979,\n",
       "  0.0019209444290027022,\n",
       "  0.0019372025271877646,\n",
       "  0.001908535836264491,\n",
       "  0.002001062035560608,\n",
       "  0.0018876470858231187,\n",
       "  0.0019175923662260175,\n",
       "  0.0019506559474393725,\n",
       "  0.0020444237161427736,\n",
       "  0.0018745922716334462,\n",
       "  0.0020079966634511948,\n",
       "  0.0018232043366879225,\n",
       "  0.0019862863700836897,\n",
       "  0.002031177980825305,\n",
       "  0.0018820997793227434,\n",
       "  0.0018886615289375186,\n",
       "  0.0018110930686816573,\n",
       "  0.001838087453506887,\n",
       "  0.0019429703243076801,\n",
       "  0.001882628770545125,\n",
       "  0.001957033760845661,\n",
       "  0.001997071085497737,\n",
       "  0.0019319131970405579,\n",
       "  0.001957222353667021,\n",
       "  0.0019457058515399694,\n",
       "  0.001867747982032597,\n",
       "  0.0019467767560854554,\n",
       "  0.0019558616913855076,\n",
       "  0.0018800345715135336,\n",
       "  0.0018690949073061347,\n",
       "  0.0019209004240110517,\n",
       "  0.001959895947948098,\n",
       "  0.0019864363130182028,\n",
       "  0.00204227352514863,\n",
       "  0.0018847738392651081,\n",
       "  0.0020335547160357237,\n",
       "  0.0018634950974956155,\n",
       "  0.001914359861984849,\n",
       "  0.001965520903468132,\n",
       "  0.001793581759557128,\n",
       "  0.0018423100700601935,\n",
       "  0.0019606605637818575,\n",
       "  0.0018224605591967702,\n",
       "  0.0020123582798987627,\n",
       "  0.0017467269208282232,\n",
       "  0.0022302712313830853,\n",
       "  0.001963676419109106,\n",
       "  0.002020046580582857,\n",
       "  0.0019328288035467267,\n",
       "  0.0019003202905878425,\n",
       "  0.0020172579679638147,\n",
       "  0.0020845700055360794,\n",
       "  0.002117687603458762,\n",
       "  0.0019249424804002047,\n",
       "  0.001985544338822365,\n",
       "  0.0017623009625822306,\n",
       "  0.0019321467261761427,\n",
       "  0.0021331633906811476,\n",
       "  0.001960726222023368,\n",
       "  0.0019474192522466183,\n",
       "  0.0019534339662641287,\n",
       "  0.0021603612694889307,\n",
       "  0.0021327752619981766,\n",
       "  0.001949236262589693,\n",
       "  0.0021096430718898773,\n",
       "  0.0018254622118547559,\n",
       "  0.0019200200913473964,\n",
       "  0.001869530649855733,\n",
       "  0.0019030269468203187,\n",
       "  0.0018995956052094698,\n",
       "  0.0020584198646247387,\n",
       "  0.0020432360470294952,\n",
       "  0.001988077536225319,\n",
       "  0.0018434582743793726,\n",
       "  0.0019845280330628157,\n",
       "  0.002047234447672963,\n",
       "  0.001990898512303829,\n",
       "  0.0019948675762861967,\n",
       "  0.0019109360873699188,\n",
       "  0.0019238142995163798,\n",
       "  0.002031448297202587,\n",
       "  0.0019631742034107447,\n",
       "  0.001963539980351925,\n",
       "  0.0018771016038954258,\n",
       "  0.0019435269059613347,\n",
       "  0.0019413838163018227,\n",
       "  0.0019265486625954509,\n",
       "  0.0019285521702840924,\n",
       "  0.0019620386883616447,\n",
       "  0.0019370155641809106,\n",
       "  0.001963134855031967,\n",
       "  0.0019194632768630981,\n",
       "  0.0019060680642724037,\n",
       "  0.001971360296010971,\n",
       "  0.0019552549347281456,\n",
       "  0.001984449103474617,\n",
       "  0.0020247383508831263,\n",
       "  0.0019879983738064766,\n",
       "  0.001950601814314723,\n",
       "  0.002082430524751544,\n",
       "  0.0018445522291585803,\n",
       "  0.0018708831630647182,\n",
       "  0.002031621988862753,\n",
       "  0.0018849443877115846],\n",
       " [0.0020135571248829365,\n",
       "  0.0019263399299234152,\n",
       "  0.0018938196590170264,\n",
       "  0.0020235960837453604,\n",
       "  0.001956494990736246,\n",
       "  0.001879786024801433,\n",
       "  0.0018580391770228744,\n",
       "  0.002010052790865302,\n",
       "  0.0019579161889851093,\n",
       "  0.00206604297272861,\n",
       "  0.0019403453916311264,\n",
       "  0.0020466679707169533,\n",
       "  0.0018007154576480389,\n",
       "  0.0018816549563780427,\n",
       "  0.00196630135178566,\n",
       "  0.001891378196887672,\n",
       "  0.0018927181372419,\n",
       "  0.002101385034620762,\n",
       "  0.0019370336085557938,\n",
       "  0.0020079808309674263,\n",
       "  0.0018897011177614331,\n",
       "  0.0019966568797826767,\n",
       "  0.0019461078336462379,\n",
       "  0.0017602273728698492,\n",
       "  0.0018204814987257123,\n",
       "  0.0018544240156188607,\n",
       "  0.0019102664664387703,\n",
       "  0.001911125029437244,\n",
       "  0.0018319099908694625,\n",
       "  0.002024649642407894,\n",
       "  0.0019746324978768826,\n",
       "  0.0019267769530415535,\n",
       "  0.0020133082289248705,\n",
       "  0.001964506460353732,\n",
       "  0.001845840597525239,\n",
       "  0.0018884694436565042,\n",
       "  0.0020109715405851603,\n",
       "  0.001964243594557047,\n",
       "  0.0019685078877955675,\n",
       "  0.0020976096857339144,\n",
       "  0.00197447557002306,\n",
       "  0.001993359997868538,\n",
       "  0.0018630215199664235,\n",
       "  0.0020728425588458776,\n",
       "  0.0019495460437610745,\n",
       "  0.0020138900727033615,\n",
       "  0.0018562872428447008,\n",
       "  0.0019691111519932747,\n",
       "  0.0017975281225517392,\n",
       "  0.0019131377339363098,\n",
       "  0.0019760672003030777,\n",
       "  0.0018087548669427633,\n",
       "  0.001937741762958467,\n",
       "  0.0020126572344452143,\n",
       "  0.001798071782104671,\n",
       "  0.0018795569194480777,\n",
       "  0.002002450404688716,\n",
       "  0.001942371716722846,\n",
       "  0.001849458203651011,\n",
       "  0.001799248973838985,\n",
       "  0.0017785654636099935,\n",
       "  0.002109582768753171,\n",
       "  0.0018039413262158632,\n",
       "  0.001885613426566124,\n",
       "  0.0019124067621305585,\n",
       "  0.001966581679880619,\n",
       "  0.0020181662403047085,\n",
       "  0.0018639983609318733,\n",
       "  0.0019395325798541307,\n",
       "  0.0018704072572290897,\n",
       "  0.0021090114023536444,\n",
       "  0.0019492495339363813,\n",
       "  0.001865992322564125,\n",
       "  0.0018327549332752824,\n",
       "  0.0019592330791056156,\n",
       "  0.0018729293951764703,\n",
       "  0.0019402735633775592,\n",
       "  0.001977832755073905,\n",
       "  0.0019532020669430494,\n",
       "  0.0019671558402478695,\n",
       "  0.001981491921469569,\n",
       "  0.001988467527553439,\n",
       "  0.002060956321656704,\n",
       "  0.0018334079068154097,\n",
       "  0.0019348625792190433,\n",
       "  0.002046256558969617,\n",
       "  0.0018761231331154704,\n",
       "  0.0019474702421575785,\n",
       "  0.00180129858199507,\n",
       "  0.0019409850938245654,\n",
       "  0.0019677181262522936,\n",
       "  0.0019050592090934515,\n",
       "  0.001923950738273561,\n",
       "  0.0017837980994954705,\n",
       "  0.001940799760632217,\n",
       "  0.00198493548668921,\n",
       "  0.002030875999480486,\n",
       "  0.0019885580986738205,\n",
       "  0.0018020400311797857,\n",
       "  0.0021785811986774206,\n",
       "  0.0019385626073926687,\n",
       "  0.0020864810794591904,\n",
       "  0.001975919818505645,\n",
       "  0.002009118441492319,\n",
       "  0.0019818120636045933,\n",
       "  0.0019856353756040335,\n",
       "  0.002000137697905302,\n",
       "  0.001959590008482337,\n",
       "  0.0019074041629210114,\n",
       "  0.0019387322245165706,\n",
       "  0.0020793003495782614,\n",
       "  0.0019035087898373604,\n",
       "  0.001898223883472383,\n",
       "  0.00196845643222332,\n",
       "  0.001987575553357601,\n",
       "  0.0020880515221506357,\n",
       "  0.0020640906877815723,\n",
       "  0.002026014495640993,\n",
       "  0.002041792031377554,\n",
       "  0.0020491122268140316,\n",
       "  0.001874934765510261,\n",
       "  0.0019519368652254343,\n",
       "  0.001916432986035943,\n",
       "  0.0019949120469391346,\n",
       "  0.0019637886434793472,\n",
       "  0.00185225042514503,\n",
       "  0.0019122569356113672,\n",
       "  0.0019048188114538789,\n",
       "  0.0018431736389175057,\n",
       "  0.0018213201547041535,\n",
       "  0.0019093425944447517,\n",
       "  0.0018155291909351945,\n",
       "  0.0019862724002450705,\n",
       "  0.0019500611815601587,\n",
       "  0.0019911061972379684,\n",
       "  0.0020528999157249928,\n",
       "  0.0019522833172231913,\n",
       "  0.0018694777972996235,\n",
       "  0.0020049498416483402,\n",
       "  0.001953791594132781,\n",
       "  0.0018980922177433968,\n",
       "  0.002034137723967433,\n",
       "  0.0018019697163254023,\n",
       "  0.002163882600143552,\n",
       "  0.002074586693197489,\n",
       "  0.0020568203181028366,\n",
       "  0.0019244940485805273,\n",
       "  0.0019803387112915516,\n",
       "  0.0019193089101463556,\n",
       "  0.001972462283447385,\n",
       "  0.0018692982848733664,\n",
       "  0.001954903593286872,\n",
       "  0.0018116756109520793,\n",
       "  0.0018586289370432496,\n",
       "  0.002192882588133216,\n",
       "  0.0019865105859935284,\n",
       "  0.001985520590096712,\n",
       "  0.002054995158687234,\n",
       "  0.001843339647166431,\n",
       "  0.0019991390872746706,\n",
       "  0.0019656287040561438,\n",
       "  0.001998667838051915,\n",
       "  0.001868639257736504,\n",
       "  0.0020558952819556,\n",
       "  0.0019124936079606414,\n",
       "  0.00213577039539814,\n",
       "  0.0018371405312791467,\n",
       "  0.001959824236109853,\n",
       "  0.0018616407178342342,\n",
       "  0.002006645081564784,\n",
       "  0.0018296866910532117,\n",
       "  0.0019176802597939968,\n",
       "  0.0019456240115687251,\n",
       "  0.002145344391465187,\n",
       "  0.001978884916752577,\n",
       "  0.0019997619092464447,\n",
       "  0.002082466147840023,\n",
       "  0.0018549921223893762,\n",
       "  0.001818573335185647,\n",
       "  0.0019005709327757359,\n",
       "  0.002069002017378807,\n",
       "  0.001868080347776413,\n",
       "  0.0020576228853315115,\n",
       "  0.002019390230998397,\n",
       "  0.0020867199636995792,\n",
       "  0.0019251341000199318,\n",
       "  0.002010099356994033,\n",
       "  0.0019878274761140347,\n",
       "  0.0022396105341613293,\n",
       "  0.0019456965383142233,\n",
       "  0.0019445524085313082,\n",
       "  0.0019475173903629184,\n",
       "  0.0020837762858718634,\n",
       "  0.002041144296526909,\n",
       "  0.0021102132741361856,\n",
       "  0.002020899672061205,\n",
       "  0.0019445773214101791,\n",
       "  0.0020722318440675735,\n",
       "  0.0020140623673796654,\n",
       "  0.002002550521865487,\n",
       "  0.0019158482318744063,\n",
       "  0.0018290153238922358,\n",
       "  0.0019984999671578407,\n",
       "  0.001911732368171215,\n",
       "  0.0020202817395329475,\n",
       "  0.002032320015132427,\n",
       "  0.002099135424941778,\n",
       "  0.0021124735940247774,\n",
       "  0.0021020302083343267,\n",
       "  0.002061749342828989,\n",
       "  0.0019181309035047889,\n",
       "  0.002007800620049238,\n",
       "  0.0019422582117840648,\n",
       "  0.0019480172777548432,\n",
       "  0.001935826032422483,\n",
       "  0.001883442746475339,\n",
       "  0.0019605611450970173,\n",
       "  0.0019112052395939827,\n",
       "  0.0019531234866008162,\n",
       "  0.001997539773583412,\n",
       "  0.0019939555786550045,\n",
       "  0.00201980653218925,\n",
       "  0.0020705019123852253,\n",
       "  0.001980312867090106,\n",
       "  0.0019130498403683305,\n",
       "  0.0019087221007794142,\n",
       "  0.001949684345163405,\n",
       "  0.0018232190050184727,\n",
       "  0.001862122560851276,\n",
       "  0.002065483946353197,\n",
       "  0.0018718467326834798,\n",
       "  0.002119647338986397,\n",
       "  0.0019677639938890934,\n",
       "  0.001897761831060052,\n",
       "  0.0018480591243132949,\n",
       "  0.0019807727076113224,\n",
       "  0.001975751481950283,\n",
       "  0.0020146698225289583,\n",
       "  0.0018893618835136294,\n",
       "  0.001980887958779931,\n",
       "  0.0019094650633633137,\n",
       "  0.001976835774257779,\n",
       "  0.0019431415712460876,\n",
       "  0.002056708326563239,\n",
       "  0.0018924538744613528,\n",
       "  0.0019815072882920504,\n",
       "  0.0020226086489856243,\n",
       "  0.0019301827996969223,\n",
       "  0.001979462569579482,\n",
       "  0.001995898550376296,\n",
       "  0.0019738683477044106,\n",
       "  0.001974572194740176,\n",
       "  0.0020734050776809454,\n",
       "  0.001874567591585219,\n",
       "  0.0019613695330917835,\n",
       "  0.001945345662534237,\n",
       "  0.0020023782271891832,\n",
       "  0.001954170875251293,\n",
       "  0.0019061834318563342,\n",
       "  0.0020178828854113817,\n",
       "  0.0019823629409074783,\n",
       "  0.0021346795838326216,\n",
       "  0.0018516230629757047,\n",
       "  0.0019438618328422308,\n",
       "  0.0018750980962067842,\n",
       "  0.002051836811006069,\n",
       "  0.0019697577226907015,\n",
       "  0.001823485130444169,\n",
       "  0.001797469798475504,\n",
       "  0.0018253843300044537,\n",
       "  0.0018083250615745783,\n",
       "  0.0019294742960482836,\n",
       "  0.001902732183225453,\n",
       "  0.001777764642611146,\n",
       "  0.0018694443861022592,\n",
       "  0.0020013442263007164,\n",
       "  0.002008972456678748,\n",
       "  0.0017957038944587111,\n",
       "  0.0021269768476486206,\n",
       "  0.0019552710000425577,\n",
       "  0.00200180453248322,\n",
       "  0.0021287822164595127,\n",
       "  0.0018785102292895317,\n",
       "  0.0019977439660578966,\n",
       "  0.0019172061001881957,\n",
       "  0.002020512707531452,\n",
       "  0.0020942743867635727,\n",
       "  0.0019741503056138754,\n",
       "  0.0020408211275935173,\n",
       "  0.001966927433386445,\n",
       "  0.0018188485410064459,\n",
       "  0.0018543163314461708,\n",
       "  0.0020397852640599012,\n",
       "  0.0020775548182427883,\n",
       "  0.0019013658165931702,\n",
       "  0.0018196168821305037,\n",
       "  0.0019861666951328516,\n",
       "  0.002009213902056217,\n",
       "  0.001949592144228518,\n",
       "  0.002021527150645852,\n",
       "  0.0018288471037521958,\n",
       "  0.0019620307721197605,\n",
       "  0.001787283574230969,\n",
       "  0.001975631108507514,\n",
       "  0.0018688481068238616,\n",
       "  0.001956903375685215,\n",
       "  0.0019680906552821398,\n",
       "  0.0019272655481472611,\n",
       "  0.0018567049410194159,\n",
       "  0.001954783918336034,\n",
       "  0.00199188687838614,\n",
       "  0.0020650855731219053,\n",
       "  0.0019625811837613583,\n",
       "  0.0019529996206983924,\n",
       "  0.001958251930773258,\n",
       "  0.0019341935403645039,\n",
       "  0.001967839663848281,\n",
       "  0.0018333933549001813,\n",
       "  0.0020294308196753263,\n",
       "  0.0020181064028292894,\n",
       "  0.0021140279714018106,\n",
       "  0.002025145571678877,\n",
       "  0.001984870061278343,\n",
       "  0.002046570647507906,\n",
       "  0.0020190568175166845,\n",
       "  0.0018603344215080142,\n",
       "  0.0020600399002432823,\n",
       "  0.0019215866923332214,\n",
       "  0.0019465460209175944,\n",
       "  0.0019562519155442715,\n",
       "  0.0019674389623105526,\n",
       "  0.00198782654479146,\n",
       "  0.0019092996371909976,\n",
       "  0.0020079067908227444,\n",
       "  0.0019013582495972514,\n",
       "  0.0021309491712599993,\n",
       "  0.0018806068692356348,\n",
       "  0.001921562128700316,\n",
       "  0.0019441071199253201,\n",
       "  0.0018427995964884758,\n",
       "  0.0018663553055375814,\n",
       "  0.0019605413544923067,\n",
       "  0.0020078958477824926,\n",
       "  0.0020732751581817865,\n",
       "  0.001897031906992197,\n",
       "  0.001913741696625948,\n",
       "  0.0019095332827419043,\n",
       "  0.001944401185028255,\n",
       "  0.001975726569071412,\n",
       "  0.0019518588669598103,\n",
       "  0.001832908601500094,\n",
       "  0.0021114815026521683,\n",
       "  0.0018584552453830838,\n",
       "  0.001964271767064929,\n",
       "  0.0019494635052978992,\n",
       "  0.0019177866633981466,\n",
       "  0.0019319638377055526,\n",
       "  0.002041673520579934,\n",
       "  0.001938850968144834,\n",
       "  0.001967206597328186,\n",
       "  0.002046587411314249,\n",
       "  0.002033414551988244,\n",
       "  0.001870801206678152,\n",
       "  0.002055969089269638,\n",
       "  0.00200660084374249,\n",
       "  0.0019753489177674055,\n",
       "  0.0019321631407365203,\n",
       "  0.0021004597656428814,\n",
       "  0.0019985425751656294,\n",
       "  0.0019523216178640723,\n",
       "  0.001958452397957444,\n",
       "  0.0017775571905076504,\n",
       "  0.0019981737714260817,\n",
       "  0.0019596132915467024,\n",
       "  0.0017984072910621762,\n",
       "  0.0018786698346957564,\n",
       "  0.0018915872788056731,\n",
       "  0.001977751264348626,\n",
       "  0.0019657458178699017,\n",
       "  0.0017914504278451204,\n",
       "  0.0020470330491662025,\n",
       "  0.001854037051089108,\n",
       "  0.0019457084126770496,\n",
       "  0.0019497221801429987,\n",
       "  0.001828793785534799,\n",
       "  0.001927312114275992,\n",
       "  0.001946561853401363,\n",
       "  0.0018361547263339162,\n",
       "  0.0019310923526063561,\n",
       "  0.0019520800560712814,\n",
       "  0.0020346243400126696,\n",
       "  0.0019375280244275928,\n",
       "  0.0019494849257171154,\n",
       "  0.0018962826579809189,\n",
       "  0.0019451366970315576,\n",
       "  0.0019915574230253696,\n",
       "  0.0019065005471929908,\n",
       "  0.001897667534649372,\n",
       "  0.0020434739999473095,\n",
       "  0.0019176116911694407,\n",
       "  0.002017244230955839,\n",
       "  0.0020210917573422194,\n",
       "  0.0018813222413882613,\n",
       "  0.002011072589084506,\n",
       "  0.002142014680430293,\n",
       "  0.001887839869596064,\n",
       "  0.0018453599186614156,\n",
       "  0.0018444207962602377,\n",
       "  0.0019951695576310158,\n",
       "  0.001942143659107387,\n",
       "  0.0018697748892009258,\n",
       "  0.001827911357395351,\n",
       "  0.001992921344935894,\n",
       "  0.0019003659253939986,\n",
       "  0.0019765240140259266,\n",
       "  0.0020328545942902565,\n",
       "  0.002056385390460491,\n",
       "  0.0019167232094332576,\n",
       "  0.00198201066814363,\n",
       "  0.0018649118719622493,\n",
       "  0.001920898910611868,\n",
       "  0.0019986426923424006,\n",
       "  0.0019636594224721193,\n",
       "  0.0019326376495882869,\n",
       "  0.0018829085165634751,\n",
       "  0.0019388003274798393,\n",
       "  0.0019336509285494685,\n",
       "  0.0018980690510943532,\n",
       "  0.0019318955019116402,\n",
       "  0.00197238870896399,\n",
       "  0.0018955186242237687,\n",
       "  0.001995193539187312,\n",
       "  0.0019696797244250774,\n",
       "  0.0018308786675333977,\n",
       "  0.0019367776112630963,\n",
       "  0.0018817564705386758,\n",
       "  0.0019011246040463448,\n",
       "  0.0018935746047645807,\n",
       "  0.001847206731326878,\n",
       "  0.0019072974100708961,\n",
       "  0.0019127900013700128,\n",
       "  0.0019405927741900086,\n",
       "  0.0018706034170463681,\n",
       "  0.0021312511526048183,\n",
       "  0.0018441245192661881,\n",
       "  0.0019257745007053018,\n",
       "  0.0020049659069627523,\n",
       "  0.0017934867646545172,\n",
       "  0.0017978327814489603,\n",
       "  0.0018216753378510475,\n",
       "  0.0018796557560563087,\n",
       "  0.002065857406705618,\n",
       "  0.0017600858118385077,\n",
       "  0.0020441412925720215,\n",
       "  0.001924741780385375,\n",
       "  0.001980450237169862,\n",
       "  0.00200136611238122,\n",
       "  0.0019626938737928867,\n",
       "  0.002006711671128869,\n",
       "  0.001990696880966425,\n",
       "  0.00215172884054482,\n",
       "  0.0020404949318617582,\n",
       "  0.0019908659160137177,\n",
       "  0.00180378882214427,\n",
       "  0.0019006389193236828,\n",
       "  0.0021215579472482204,\n",
       "  0.0019191446481272578,\n",
       "  0.0020245870109647512,\n",
       "  0.002002725610509515,\n",
       "  0.002006485126912594,\n",
       "  0.0021405003499239683,\n",
       "  0.0019623904954642057,\n",
       "  0.0020349256228655577,\n",
       "  0.0018812663620337844,\n",
       "  0.0019501886563375592,\n",
       "  0.0018194210715591908,\n",
       "  0.0019531227881088853,\n",
       "  0.0018975657876580954,\n",
       "  0.002048755530267954,\n",
       "  0.0021468154154717922,\n",
       "  0.0019110202556475997,\n",
       "  0.001896602101624012,\n",
       "  0.0019916887395083904,\n",
       "  0.0019918226171284914,\n",
       "  0.002079949015751481,\n",
       "  0.002054463606327772,\n",
       "  0.0018908681813627481,\n",
       "  0.0018259768839925528,\n",
       "  0.0020586038008332253,\n",
       "  0.0020075009670108557,\n",
       "  0.0019744914025068283,\n",
       "  0.0018692536978051066,\n",
       "  0.0019039181061089039,\n",
       "  0.0018853957299143076,\n",
       "  0.001903264201246202,\n",
       "  0.0018559300806373358,\n",
       "  0.002040296094492078,\n",
       "  0.0018970167730003595,\n",
       "  0.0019165267003700137,\n",
       "  0.001991466386243701,\n",
       "  0.0018996027065441012,\n",
       "  0.0019690273329615593,\n",
       "  0.001980237429961562,\n",
       "  0.002025703899562359,\n",
       "  0.001982151297852397,\n",
       "  0.0020870182197541,\n",
       "  0.001908698002807796,\n",
       "  0.0020646231714636087,\n",
       "  0.001878398354165256,\n",
       "  0.0018033955711871386,\n",
       "  0.0018989271484315395,\n",
       "  0.0019149777945131063],\n",
       " [0.0021222445648163557,\n",
       "  0.0019049800466746092,\n",
       "  0.0018729420844465494,\n",
       "  0.002029979834333062,\n",
       "  0.0018153104465454817,\n",
       "  0.0017753058345988393,\n",
       "  0.0018081036396324635,\n",
       "  0.002045164816081524,\n",
       "  0.0019952929578721523,\n",
       "  0.0019413548288866878,\n",
       "  0.001976641360670328,\n",
       "  0.0020403603557497263,\n",
       "  0.0018388573080301285,\n",
       "  0.001891312887892127,\n",
       "  0.001991693861782551,\n",
       "  0.0019254852086305618,\n",
       "  0.0019574076868593693,\n",
       "  0.0020736991427838802,\n",
       "  0.0019254466751590371,\n",
       "  0.002077053766697645,\n",
       "  0.0020368206314742565,\n",
       "  0.001967726042494178,\n",
       "  0.001890620100311935,\n",
       "  0.0019174041226506233,\n",
       "  0.0018353840569034219,\n",
       "  0.001913410727865994,\n",
       "  0.0019764378666877747,\n",
       "  0.0020095373038202524,\n",
       "  0.0019364567706361413,\n",
       "  0.00203169253654778,\n",
       "  0.0021122079342603683,\n",
       "  0.001923818956129253,\n",
       "  0.002028358168900013,\n",
       "  0.002035346580669284,\n",
       "  0.0018323890399187803,\n",
       "  0.0019182543037459254,\n",
       "  0.0018399797845631838,\n",
       "  0.002028818940743804,\n",
       "  0.0020991505589336157,\n",
       "  0.002164633944630623,\n",
       "  0.001977411564439535,\n",
       "  0.0019086168613284826,\n",
       "  0.0018898557173088193,\n",
       "  0.0020351328421384096,\n",
       "  0.0019343108870089054,\n",
       "  0.002051541581749916,\n",
       "  0.0018177772872149944,\n",
       "  0.0019315109821036458,\n",
       "  0.0018612199928611517,\n",
       "  0.0019931504502892494,\n",
       "  0.001985041657462716,\n",
       "  0.0019527284894138575,\n",
       "  0.00198230124078691,\n",
       "  0.0019917224999517202,\n",
       "  0.0018066419288516045,\n",
       "  0.0018883703742176294,\n",
       "  0.002055897144600749,\n",
       "  0.0018547795480117202,\n",
       "  0.0019176141358911991,\n",
       "  0.0018980192253366113,\n",
       "  0.001730958465486765,\n",
       "  0.002135765738785267,\n",
       "  0.001850649481639266,\n",
       "  0.0019826439674943686,\n",
       "  0.002026668982580304,\n",
       "  0.0019878507591784,\n",
       "  0.001947427517734468,\n",
       "  0.0019042795756831765,\n",
       "  0.0018975953571498394,\n",
       "  0.0018139536259695888,\n",
       "  0.0021289163269102573,\n",
       "  0.0019595238845795393,\n",
       "  0.001852225512266159,\n",
       "  0.0018993581179529428,\n",
       "  0.001950980513356626,\n",
       "  0.001895581721328199,\n",
       "  0.001860510790720582,\n",
       "  0.0019709612242877483,\n",
       "  0.0018258250784128904,\n",
       "  0.0019151041051372886,\n",
       "  0.00196642242372036,\n",
       "  0.0019648903980851173,\n",
       "  0.001994380494579673,\n",
       "  0.0019397757714614272,\n",
       "  0.0019778457935899496,\n",
       "  0.0018897956470027566,\n",
       "  0.0019789389334619045,\n",
       "  0.001977487001568079,\n",
       "  0.0018451696960255504,\n",
       "  0.0019418438896536827,\n",
       "  0.0019452766282483935,\n",
       "  0.0019261672860011458,\n",
       "  0.001864554942585528,\n",
       "  0.0018298859940841794,\n",
       "  0.0019594875629991293,\n",
       "  0.0019452419364824891,\n",
       "  0.001955245388671756,\n",
       "  0.0019123622914776206,\n",
       "  0.001786248292773962,\n",
       "  0.002080047968775034,\n",
       "  0.0019261136185377836,\n",
       "  0.0021566145587712526,\n",
       "  0.0019739544950425625,\n",
       "  0.0020279602613300085,\n",
       "  0.002071014605462551,\n",
       "  0.0019540423527359962,\n",
       "  0.0020106500014662743,\n",
       "  0.002044757129624486,\n",
       "  0.001993965357542038,\n",
       "  0.0020173382945358753,\n",
       "  0.002108938293531537,\n",
       "  0.0019189484883099794,\n",
       "  0.001876218942925334,\n",
       "  0.001998407766222954,\n",
       "  0.002046687761321664,\n",
       "  0.0019986978732049465,\n",
       "  0.0019026234513148665,\n",
       "  0.001997111365199089,\n",
       "  0.0019284711452201009,\n",
       "  0.001993210054934025,\n",
       "  0.0019065948436036706,\n",
       "  0.0020687696523964405,\n",
       "  0.0018468574853613973,\n",
       "  0.0019099231576547027,\n",
       "  0.0018480687867850065,\n",
       "  0.0018978595035150647,\n",
       "  0.0018459397833794355,\n",
       "  0.0018966449424624443,\n",
       "  0.0019415220012888312,\n",
       "  0.001839642645791173,\n",
       "  0.001874819747172296,\n",
       "  0.001892576925456524,\n",
       "  0.0019359359284862876,\n",
       "  0.0019416228169575334,\n",
       "  0.002022214699536562,\n",
       "  0.0020677479915320873,\n",
       "  0.0019025212386623025,\n",
       "  0.0018667066469788551,\n",
       "  0.001972277881577611,\n",
       "  0.0019664508290588856,\n",
       "  0.002024928107857704,\n",
       "  0.0020081724505871534,\n",
       "  0.0018579966854304075,\n",
       "  0.002016872400417924,\n",
       "  0.002081468468531966,\n",
       "  0.0019608435686677694,\n",
       "  0.0018931376980617642,\n",
       "  0.0020297104492783546,\n",
       "  0.0019384222105145454,\n",
       "  0.001911825849674642,\n",
       "  0.0019297057297080755,\n",
       "  0.00202131737023592,\n",
       "  0.001950861536897719,\n",
       "  0.001896236790344119,\n",
       "  0.0021185465157032013,\n",
       "  0.0020152078941464424,\n",
       "  0.0019859529566019773,\n",
       "  0.0019818691071122885,\n",
       "  0.0018707993440330029,\n",
       "  0.0019211032195016742,\n",
       "  0.0019233508501201868,\n",
       "  0.0020647060591727495,\n",
       "  0.0019161858363077044,\n",
       "  0.0020657519344240427,\n",
       "  0.002003985457122326,\n",
       "  0.0020675514824688435,\n",
       "  0.0018773638876155019,\n",
       "  0.002048049820587039,\n",
       "  0.0018226642860099673,\n",
       "  0.001941895461641252,\n",
       "  0.0018240830395370722,\n",
       "  0.0019417962757870555,\n",
       "  0.0019140270305797458,\n",
       "  0.0020665996707975864,\n",
       "  0.002067657420411706,\n",
       "  0.002017970895394683,\n",
       "  0.0019433556590229273,\n",
       "  0.0018066224874928594,\n",
       "  0.0019433776615187526,\n",
       "  0.0019021624466404319,\n",
       "  0.0019729381892830133,\n",
       "  0.0018491687951609492,\n",
       "  0.002017201157286763,\n",
       "  0.0019276330713182688,\n",
       "  0.00200597639195621,\n",
       "  0.001959578599780798,\n",
       "  0.001915181172080338,\n",
       "  0.0019545177929103374,\n",
       "  0.0020248745568096638,\n",
       "  0.0019162432290613651,\n",
       "  0.0019808164797723293,\n",
       "  0.0018582275370135903,\n",
       "  0.002076133154332638,\n",
       "  0.001987824682146311,\n",
       "  0.0020901833195239305,\n",
       "  0.0019445643993094563,\n",
       "  0.0018739482620730996,\n",
       "  0.0020476561039686203,\n",
       "  0.0020369011908769608,\n",
       "  0.0020109761971980333,\n",
       "  0.00194125238340348,\n",
       "  0.0019461121410131454,\n",
       "  0.0019700261764228344,\n",
       "  0.0019782723393291235,\n",
       "  0.0020969309844076633,\n",
       "  0.001959666609764099,\n",
       "  0.0020450097508728504,\n",
       "  0.0019858686719089746,\n",
       "  0.0020172304939478636,\n",
       "  0.0020430628210306168,\n",
       "  0.001990559045225382,\n",
       "  0.0021026807371526957,\n",
       "  0.0018765106797218323,\n",
       "  0.002026156522333622,\n",
       "  0.0019528212724253535,\n",
       "  0.001852776505984366,\n",
       "  0.0019791130907833576,\n",
       "  0.001949333120137453,\n",
       "  0.0020672159735113382,\n",
       "  0.0018431299831718206,\n",
       "  0.0020180107094347477,\n",
       "  0.001954710576683283,\n",
       "  0.001969739329069853,\n",
       "  0.001983129419386387,\n",
       "  0.0019558356143534184,\n",
       "  0.0018147332593798637,\n",
       "  0.0020032173488289118,\n",
       "  0.0019659698009490967,\n",
       "  0.0018369309836998582,\n",
       "  0.0020199522841721773,\n",
       "  0.0018844474107027054,\n",
       "  0.0019548428244888783,\n",
       "  0.0020924443379044533,\n",
       "  0.0018442918080836535,\n",
       "  0.0019032165873795748,\n",
       "  0.0019209691090509295,\n",
       "  0.001903778174892068,\n",
       "  0.0021436933893710375,\n",
       "  0.0019228854216635227,\n",
       "  0.0018897871486842632,\n",
       "  0.0019394243136048317,\n",
       "  0.001964624272659421,\n",
       "  0.0021142626646906137,\n",
       "  0.001983023714274168,\n",
       "  0.0019825659692287445,\n",
       "  0.002099355449900031,\n",
       "  0.0019966515246778727,\n",
       "  0.0020530088804662228,\n",
       "  0.0019272823119536042,\n",
       "  0.0019492942374199629,\n",
       "  0.0018687251722440124,\n",
       "  0.001966811716556549,\n",
       "  0.0020879267249256372,\n",
       "  0.0019688373431563377,\n",
       "  0.001960537163540721,\n",
       "  0.0019437363371253014,\n",
       "  0.0021081736776977777,\n",
       "  0.0018995769787579775,\n",
       "  0.0019448018865659833,\n",
       "  0.0020538405515253544,\n",
       "  0.0019207051955163479,\n",
       "  0.001916262204758823,\n",
       "  0.0017880410887300968,\n",
       "  0.0018438416300341487,\n",
       "  0.0018975178245455027,\n",
       "  0.0019013414857909083,\n",
       "  0.0019777112174779177,\n",
       "  0.0018530215602368116,\n",
       "  0.001791969407349825,\n",
       "  0.0019136731280013919,\n",
       "  0.0018437070539221168,\n",
       "  0.0019080726196989417,\n",
       "  0.002014162251725793,\n",
       "  0.001889978302642703,\n",
       "  0.0019756359979510307,\n",
       "  0.0020519327372312546,\n",
       "  0.0019346224144101143,\n",
       "  0.001818727352656424,\n",
       "  0.0019325459143146873,\n",
       "  0.0019494070438668132,\n",
       "  0.0018899866845458746,\n",
       "  0.0019651358015835285,\n",
       "  0.001980237662792206,\n",
       "  0.001970317680388689,\n",
       "  0.0019231666810810566,\n",
       "  0.0020801208447664976,\n",
       "  0.0020947500597685575,\n",
       "  0.0018989528762176633,\n",
       "  0.00197596219368279,\n",
       "  0.0020388534758239985,\n",
       "  0.0018879721174016595,\n",
       "  0.0019009497482329607,\n",
       "  0.0020465096458792686,\n",
       "  0.0019318177364766598,\n",
       "  0.0019120380748063326,\n",
       "  0.0017779931658878922,\n",
       "  0.0020204568281769753,\n",
       "  0.0019416575087234378,\n",
       "  0.001980756875127554,\n",
       "  0.002020331099629402,\n",
       "  0.0018840247066691518,\n",
       "  0.0018188855610787868,\n",
       "  0.001759530627168715,\n",
       "  0.00188025226816535,\n",
       "  0.001818948658183217,\n",
       "  0.00194888177793473,\n",
       "  0.0019343862077221274,\n",
       "  0.001953922212123871,\n",
       "  0.0020439564250409603,\n",
       "  0.001920883427374065,\n",
       "  0.001955515705049038,\n",
       "  0.0019169639563187957,\n",
       "  0.0019374756375327706,\n",
       "  0.0019932875875383615,\n",
       "  0.0018636333988979459,\n",
       "  0.0019212416373193264,\n",
       "  0.0020522328559309244,\n",
       "  0.0017849992727860808,\n",
       "  0.0022180925589054823,\n",
       "  0.0019806991331279278,\n",
       "  0.0019316021353006363,\n",
       "  0.0019186167046427727,\n",
       "  0.0019434768473729491,\n",
       "  0.0019534495659172535,\n",
       "  0.0020291891414672136,\n",
       "  0.0019071055576205254,\n",
       "  0.002038962207734585,\n",
       "  0.0020153175573796034,\n",
       "  0.0019788797944784164,\n",
       "  0.002005206188187003,\n",
       "  0.001956057967618108,\n",
       "  0.0019493559375405312,\n",
       "  0.0018641221104189754,\n",
       "  0.002033927943557501,\n",
       "  0.002049303613603115,\n",
       "  0.0020792712457478046,\n",
       "  0.0018299897201359272,\n",
       "  0.001869273604825139,\n",
       "  0.001907021040096879,\n",
       "  0.0019123706733807921,\n",
       "  0.0018358028028160334,\n",
       "  0.0018249304266646504,\n",
       "  0.002069724490866065,\n",
       "  0.0020146153401583433,\n",
       "  0.0020032005850225687,\n",
       "  0.0018863504519686103,\n",
       "  0.002036967081949115,\n",
       "  0.001976804109290242,\n",
       "  0.0019778821151703596,\n",
       "  0.0020622978918254375,\n",
       "  0.0018941713497042656,\n",
       "  0.0020597612019628286,\n",
       "  0.0018571412656456232,\n",
       "  0.002024248009547591,\n",
       "  0.0019980682991445065,\n",
       "  0.00190911500249058,\n",
       "  0.0018355082720518112,\n",
       "  0.00200479943305254,\n",
       "  0.0019405055791139603,\n",
       "  0.0019707034807652235,\n",
       "  0.0020344152580946684,\n",
       "  0.001981043489649892,\n",
       "  0.0019407601794227958,\n",
       "  0.0019718415569514036,\n",
       "  0.0020165792666375637,\n",
       "  0.001997348153963685,\n",
       "  0.001937768654897809,\n",
       "  0.0021332083269953728,\n",
       "  0.0020077514927834272,\n",
       "  0.0018221820937469602,\n",
       "  0.001962055219337344,\n",
       "  0.0018167805392295122,\n",
       "  0.0020168356131762266,\n",
       "  0.0018892701482400298,\n",
       "  0.0019031737465411425,\n",
       "  0.0018900675931945443,\n",
       "  0.001960882218554616,\n",
       "  0.002033068798482418,\n",
       "  0.0019620086532086134,\n",
       "  0.0018174471333622932,\n",
       "  0.0019531650468707085,\n",
       "  0.0019091854337602854,\n",
       "  0.0018516498384997249,\n",
       "  0.0019875066354870796,\n",
       "  0.0018075818661600351,\n",
       "  0.0019063377985730767,\n",
       "  0.0019674168433994055,\n",
       "  0.0018836030503734946,\n",
       "  0.0019309030612930655,\n",
       "  0.0019023437052965164,\n",
       "  0.002035125158727169,\n",
       "  0.0019873864948749542,\n",
       "  0.001896871137432754,\n",
       "  0.001867899438366294,\n",
       "  0.0019427507650107145,\n",
       "  0.0019511719001457095,\n",
       "  0.001826468389481306,\n",
       "  0.0019586102571338415,\n",
       "  0.0019891196861863136,\n",
       "  0.0019006774527952075,\n",
       "  0.002201792551204562,\n",
       "  0.0019826595671474934,\n",
       "  0.001993249636143446,\n",
       "  0.0019796169362962246,\n",
       "  0.002059089718386531,\n",
       "  0.0019106781110167503,\n",
       "  0.0019065674860030413,\n",
       "  0.001910608378238976,\n",
       "  0.0018851119093596935,\n",
       "  0.0018609887920320034,\n",
       "  0.00194651132915169,\n",
       "  0.001904231496155262,\n",
       "  0.0020217543933540583,\n",
       "  0.0019166667480021715,\n",
       "  0.0019223697017878294,\n",
       "  0.001913881511427462,\n",
       "  0.002049120143055916,\n",
       "  0.0018649958074092865,\n",
       "  0.0019511578138917685,\n",
       "  0.0018756381468847394,\n",
       "  0.001992747886106372,\n",
       "  0.002076679142192006,\n",
       "  0.001868960214778781,\n",
       "  0.0018925460753962398,\n",
       "  0.0018144184723496437,\n",
       "  0.0018353054765611887,\n",
       "  0.0019725793972611427,\n",
       "  0.0018811896443367004,\n",
       "  0.002010650932788849,\n",
       "  0.0019746709149330854,\n",
       "  0.001937494846060872,\n",
       "  0.0019646044820547104,\n",
       "  0.001930121798068285,\n",
       "  0.0018704039976000786,\n",
       "  0.00196349760517478,\n",
       "  0.001940324204042554,\n",
       "  0.0018924028845503926,\n",
       "  0.0019088101107627153,\n",
       "  0.0019527459517121315,\n",
       "  0.0019663525745272636,\n",
       "  0.001963504124432802,\n",
       "  0.0020490875467658043,\n",
       "  0.0019004836212843657,\n",
       "  0.0020332476124167442,\n",
       "  0.0018616723828017712,\n",
       "  0.0019208939047530293,\n",
       "  0.0019933548755943775,\n",
       "  0.0018167634261772037,\n",
       "  0.001850363565608859,\n",
       "  0.0019782697781920433,\n",
       "  0.001821091864258051,\n",
       "  0.0019878465682268143,\n",
       "  0.0018061609007418156,\n",
       "  0.0022476110607385635,\n",
       "  0.0019046940142288804,\n",
       "  0.002059587510302663,\n",
       "  0.0019450667314231396,\n",
       "  0.001938625588081777,\n",
       "  0.0019785347394645214,\n",
       "  0.0020822114311158657,\n",
       "  0.002068910049274564,\n",
       "  0.0018747823778539896,\n",
       "  0.001998423831537366,\n",
       "  0.001768791931681335,\n",
       "  0.0019269307376816869,\n",
       "  0.00210014171898365,\n",
       "  0.0019376894924789667,\n",
       "  0.0019361797021701932,\n",
       "  0.001946819480508566,\n",
       "  0.002137165516614914,\n",
       "  0.0021821577101945877,\n",
       "  0.002003553556278348,\n",
       "  0.0021195898298174143,\n",
       "  0.0018217022297903895,\n",
       "  0.00190852046944201,\n",
       "  0.0019078938057646155,\n",
       "  0.0019103920785710216,\n",
       "  0.001913116779178381,\n",
       "  0.0020717098377645016,\n",
       "  0.0019726944155991077,\n",
       "  0.002004201291128993,\n",
       "  0.0018401936395093799,\n",
       "  0.0019705987069755793,\n",
       "  0.0020259027369320393,\n",
       "  0.001958421664312482,\n",
       "  0.0019342636223882437,\n",
       "  0.0019236395601183176,\n",
       "  0.001963650109246373,\n",
       "  0.0020146574825048447,\n",
       "  0.0019226477015763521,\n",
       "  0.001997385174036026,\n",
       "  0.001862335135228932,\n",
       "  0.0020182994194328785,\n",
       "  0.0019292377401143312,\n",
       "  0.0019166526617482305,\n",
       "  0.001940342364832759,\n",
       "  0.0019487582612782717,\n",
       "  0.0019013747805729508,\n",
       "  0.001986913848668337,\n",
       "  0.001898978604003787,\n",
       "  0.0019201439572498202,\n",
       "  0.0020041281823068857,\n",
       "  0.001923021744005382,\n",
       "  0.0020255353301763535,\n",
       "  0.0020057279616594315,\n",
       "  0.001976548694074154,\n",
       "  0.001959105720743537,\n",
       "  0.0020810721907764673,\n",
       "  0.001811749767512083,\n",
       "  0.0018782744882628322,\n",
       "  0.0020578810945153236,\n",
       "  0.0019024297362193465]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_faceslist():\n",
    "    embeds = torch.load(DATA_PATH+'/faceslist.pth')\n",
    "    names = np.load(DATA_PATH+'/usernames.npy')\n",
    "    print(embeds, names)\n",
    "    return embeds, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, face, local_embeds, threshold = 0.7,power = pow(10, 2)):\n",
    "    #local: [n,512] voi n la so nguoi trong faceslist\n",
    "    embeds = []\n",
    "    # print(trans(face).unsqueeze(0).shape)\n",
    "    embeds.append(model(trans(face).to(device).unsqueeze(0)))\n",
    "    detect_embeds = torch.cat(embeds) #[1,512]\n",
    "    print(detect_embeds.shape)\n",
    "    # print(detect_embeds.shape)\n",
    "                    #[1,512,1]                                      [1,512,n]\n",
    "    norm_diff = detect_embeds.unsqueeze(-1) - torch.transpose(local_embeds, 0, 1).unsqueeze(0)\n",
    "    # print(norm_diff)\n",
    "    norm_score = torch.sqrt(torch.sum(torch.pow(norm_diff, 2), dim=1)).tolist()[0] #(1,n), moi cot la tong khoang cach euclide so vs embed moi\n",
    "\n",
    "    min_dist, embed_idx = min(norm_score), norm_score.index(min(norm_score))\n",
    "    \n",
    "    # print(min_dist.shape)\n",
    "    if min_dist*power > threshold:\n",
    "        return -1, -1\n",
    "    else:\n",
    "        return embed_idx, min_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_face(box, img, margin=20):\n",
    "    face_size = 160\n",
    "    img_size = (WIDTH,HEIGHT)\n",
    "    margin = [\n",
    "        margin * (box[2] - box[0]) / (face_size - margin),\n",
    "        margin * (box[3] - box[1]) / (face_size - margin),\n",
    "    ] \n",
    "\n",
    "    box = [ #box[0] v box[1] l ta  ca im gc trn cng tri\n",
    "        int(max(box[0] - margin[0] / 2, 0)), #nu thm vo margin b ra khi ra nh => a v im 0\n",
    "        int(max(box[1] - margin[1] / 2, 0)),\n",
    "        int(min(box[2] + margin[0] / 2, img_size[0])), #nu thm vo margin b ra khi ra nh => a v ta  ca nh gc\n",
    "        int(min(box[3] + margin[1] / 2, img_size[1])),\n",
    "        \n",
    "    ] #to margin mi bao quanh box c\n",
    "    # img = img[box[1]:box[3], box[0]:box[2]]\n",
    "    dst_points = np.float32([[box[0], box[1]], [box[2], box[1]],  [box[2], box[3]],[box[0], box[3]]])\n",
    "    src_points = np.float32([[0, 0],             [WIDTH, 0],          [WIDTH, HEIGHT],   [0, HEIGHT]])\n",
    "\n",
    "    perspective_matrix = cv2.getPerspectiveTransform(dst_points,src_points)\n",
    "    img = cv2.warpPerspective(img, perspective_matrix, (WIDTH, HEIGHT))\n",
    "    face = cv2.resize(img,(face_size, face_size), interpolation=cv2.INTER_AREA)\n",
    "    face = Image.fromarray(face)\n",
    "    return face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0021, 0.0019, 0.0019,  ..., 0.0019, 0.0020, 0.0019],\n",
      "        [0.0021, 0.0019, 0.0019,  ..., 0.0019, 0.0020, 0.0019],\n",
      "        [0.0020, 0.0019, 0.0019,  ..., 0.0018, 0.0019, 0.0019],\n",
      "        [0.0021, 0.0019, 0.0019,  ..., 0.0019, 0.0021, 0.0019]],\n",
      "       device='cuda:0') ['Gia' 'Hoang' 'Nam' 'Tuan']\n",
      "tensor([[0.0021, 0.0019, 0.0019,  ..., 0.0019, 0.0020, 0.0019],\n",
      "        [0.0021, 0.0019, 0.0019,  ..., 0.0019, 0.0020, 0.0019],\n",
      "        [0.0020, 0.0019, 0.0019,  ..., 0.0018, 0.0019, 0.0019],\n",
      "        [0.0021, 0.0019, 0.0019,  ..., 0.0019, 0.0021, 0.0019]],\n",
      "       device='cuda:0') ['Gia' 'Hoang' 'Nam' 'Tuan']\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "embeddings, names = load_faceslist()\n",
    "print(embeddings, names )\n",
    "power = pow(10,2)\n",
    "mtcnn = MTCNN(min_face_size=80, keep_all=True,thresholds= [0.7, 0.7, 0.8], select_largest = True, post_process=False, device = device)\n",
    "\n",
    "VID_SRC = r'./data/vid/'\n",
    "Source = input(\"Source: \")\n",
    "if(Source=='cam'):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "else : \n",
    "    VID_PATH = os.path.join(VID_SRC, Source+'.mp4')\n",
    "    cap = cv2.VideoCapture(VID_PATH)\n",
    "\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH,WIDTH)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT,HEIGHT)\n",
    "while cap.isOpened():\n",
    "        isSuccess, frame = cap.read()\n",
    "        if isSuccess ==False:break\n",
    "        else:\n",
    "            boxes, _ = mtcnn.detect(frame)\n",
    "            if boxes is not None:\n",
    "                for box in boxes:\n",
    "                    bbox = list(map(int,box.tolist()))\n",
    "                    face = extract_face(bbox, frame)\n",
    "                    idx, score = inference(model, face, embeddings)\n",
    "                    if idx != -1:\n",
    "                        frame = cv2.rectangle(frame, (bbox[0],bbox[1]), (bbox[2],bbox[3]), (0,0,255), 6)\n",
    "                        score = score*power\n",
    "                        frame = cv2.putText(frame, names[idx] + '_{:.2f}'.format(score), (bbox[0],bbox[1]), cv2.FONT_HERSHEY_DUPLEX, 2, (0,255,0), 2, cv2.LINE_8)\n",
    "                    else:\n",
    "                        frame = cv2.rectangle(frame, (bbox[0],bbox[1]), (bbox[2],bbox[3]), (0,0,255), 6)\n",
    "                        frame = cv2.putText(frame,'Unknown', (bbox[0],bbox[1]), cv2.FONT_HERSHEY_DUPLEX, 2, (0,255,0), 2, cv2.LINE_8)\n",
    "            new_frame_time = time.time()\n",
    "            fps = 1/(new_frame_time-prev_frame_time)\n",
    "            prev_frame_time = new_frame_time\n",
    "            fps = str(int(fps))\n",
    "            cv2.putText(frame, fps, (7, 70), cv2.FONT_HERSHEY_DUPLEX, 3, (100, 255, 0), 3, cv2.LINE_AA)\n",
    "        cv2.imshow('Face Recognition', frame)\n",
    "        if cv2.waitKey(1)&0xFF == 27:\n",
    "            break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtcnntorchgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
